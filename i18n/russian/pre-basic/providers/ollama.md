
{% hint style="warning" %}
Этот документ переведен с китайского языка с помощью ИИ и еще не был проверен.
{% endhint %}

# Ollama

Ollama — это превосходный инструмент с открытым исходным кодом, позволяющий легко запускать и управлять различными большими языковыми моделями (LLMs) на локальном устройстве. Cherry Studio теперь поддерживает интеграцию с Ollama, что даёт возможность взаимодействовать с локально развёрнутой LLM через знакомый интерфейс без зависимости от облачных сервисов!

## Что такое Ollama?

Ollama — это инструмент для упрощённого развёртывания и использования больших языковых моделей (LLM). Его ключевые особенности:

* **Локальный запуск:** Модели работают полностью на вашем локальном компьютере без подключения к интернету, обеспечивая конфиденциальность и безопасность данных.
* **Простота использования:** Загрузка, запуск и управление моделями осуществляется простыми командами терминала.
* **Богатый выбор моделей:** Поддерживает популярные модели с открытым исходным кодом: Llama 2, Deepseek, Mistral, Gemma и другие.
* **Кросс-платформенность:** Работает в macOS, Windows и Linux.
* **Открытый API:** Совместим с OpenAI API, что позволяет интегрировать его с другими инструментами.

## Зачем использовать Ollama в Cherry Studio?

* **Без облачных сервисов:** Свободно используйте возможности локальных LLM без ограничений квот и платы за облачные API.
* **Конфиденциальность данных:** Все диалоговые данные остаются на вашем устройстве.
* **Работа офлайн:** Взаимодействуйте с моделями без интернет-соединения.
* **Кастомизация:** Выбирайте и настраивайте LLM, наиболее соответствующие вашим задачам.

## Настройка Ollama в Cherry Studio

### **1. Установка и запуск Ollama**

Сначала установите и запустите Ollama на вашем компьютере:

* **Скачивание Ollama:** Перейдите на [официальный сайт](https://ollama.com/) и скачайте установщик для вашей ОС.  
  В Linux установку можно выполнить командой:
  ```sh
  curl -fsSL https://ollama.com/install.sh | sh
  ```
* **Установка Ollama:** Следуйте инструкциям установщика.
* **Загрузка модели:** В терминале скачайте нужную модель командой `ollama run`. Например, для модели Llama 3:
  ```sh
  ollama run llama3.2
  ```
  Ollama автоматически скачает и запустит модель.
* **Поддержка работы:** Обеспечьте непрерывную работу Ollama во время взаимодействия с Cherry Studio.

### **2. Добавление Ollama как провайдера в Cherry Studio**

Добавьте Ollama в качестве пользовательского AI-провайдера:

* **Откройте настройки:** В левой панели Cherry Studio нажмите "Настройки" (иконка шестерёнки).
* **Перейдите в "Модели":** В настройках выберите вкладку "Модельные сервисы".
* **Добавьте провайдера:** Выберите Ollama в списке.

<figure><img src="../../.gitbook/assets/image (5) (3).png" alt=""><figcaption></figcaption></figure>

### **3. Конфигурация Ollama**

Настройте добавленного провайдера Ollama:

1. **Статус активности:**
   * Активируйте переключатель справа от Ollama.
2. **API-ключ:**
   * Для Ollama **не требуется** API-ключ. Оставьте поле пустым или введите любое значение.
3. **API-адрес:**
   *   Укажите локальный адрес Ollama. Стандартное значение:
       ```
       http://localhost:11434/
       ```
       (измените порт при необходимости).
4. **Время активности:** Укажите время (в минутах), через которое Cherry Studio автоматически разорвёт соединение с Ollama при отсутствии активности.
5. **Управление моделями:**
   * Нажмите "+ Добавить" для ручного ввода названия скачанной модели.
   * Например, для модели `llama3.2`, скачанной командой `ollama run llama3.2`, введите `llama3.2`.
   * Используйте кнопку "Управление" для редактирования или удаления моделей.

## Начало работы

После настройки выберите Ollama как провайдера и нужную модель в интерфейсе чата Cherry Studio для взаимодействия с локальной LLM!

## Советы

* **Первый запуск модели:** При первом запуске Ollama скачивает файлы модели — это может занять время.
* **Список моделей:** Команда `ollama list` в терминале покажет скачанные модели.
* **Аппаратные требования:** Убедитесь, что ваше устройство соответствует системным требованиям LLM (CPU, RAM, GPU).
* **Документация Ollama:** Используйте ссылку `Просмотр документации и моделей Ollama` в настройках для быстрого перехода на официальный сайт.