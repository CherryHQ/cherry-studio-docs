
{% hint style="warning" %}
Этот документ переведен с китайского языка с помощью ИИ и еще не был проверен.
{% endhint %}

# Ollama

Ollama — это превосходный инструмент с открытым исходным кодом, позволяющий вам легко запускать и управлять различными большими языковыми моделями (LLM) локально на вашем устройстве. Cherry Studio теперь поддерживает интеграцию с Ollama, позволяя взаимодействовать с локально развернутыми LLM через знакомый интерфейс без необходимости в облачных сервисах!

## Что такое Ollama?

Ollama — инструмент для упрощения развертывания и использования больших языковых моделей (LLM). Его ключевые особенности:

* **Локальный запуск:** Модели полностью работают на вашем устройстве без подключения к интернету, обеспечивая приватность и безопасность данных.
* **Простота использования:** Скачивание, запуск и управление моделями через простые команды терминала.
* **Широкий выбор моделей:** Поддержка популярных open-source моделей: Llama 2, Deepseek, Mistral, Gemma и других.
* **Кроссплатформенность:** Работает на macOS, Windows и Linux.
* **Открытый API:** Совместимый с OpenAI интерфейс для интеграции с другими инструментами.

## Преимущества использования Ollama в Cherry Studio

* **Не требует облачных сервисов:** Неограниченное использование без квот API и облачных расходов.
* **Конфиденциальность данных:** Все диалоговые данные остаются на вашем устройстве.
* **Работа офлайн:** Возможность взаимодействовать с LLM без интернет-соединения.
* **Гибкая настройка:** Выбор и конфигурация моделей под ваши специфические задачи.

## Настройка Ollama в Cherry Studio

### **1. Установка и запуск Ollama**

Сначала установите Ollama на вашем компьютере:

*   **Скачивание:** Посетите [официальный сайт](https://ollama.com/) и скачайте версию для вашей ОС.  
    Для Linux установка через терминал:
    ```sh
    curl -fsSL https://ollama.com/install.sh | sh
    ```
* **Установка:** Следуйте инструкциям установщика.
*   **Загрузка модели:** В терминале выполните команду для скачивания модели. Пример для Llama 3.2:
    ```sh
    ollama run llama3.2
    ```
* **Поддерживайте запущенный Ollama во время работы с Cherry Studio.**

### **2. Добавление Ollama как провайдера в Cherry Studio**

1. Откройте **Настройки** (иконка шестеренки в левом меню)
2. Перейдите в раздел **Сервис моделей**
3. Нажмите **Ollama** в списке провайдеров

<figure><img src="../../.gitbook/assets/image (5) (3).png" alt=""><figcaption></figcaption></figure>

### **3. Конфигурация Ollama**

Найдите добавленного провайдера Ollama и настройте:

1. **Статус активации:**  
   Включите переключатель справа
2. **API-ключ:**  
   Оставьте поле пустым (ключ не требуется)
3. **API-адрес:**  
   Стандартный адрес (при необходимости измените порт):
   ```
   http://localhost:11434/
   ```
4. **Время активности:**  
   Установите период неактивности (в минутах) до автоотключения
5. **Управление моделями:**
   * Добавьте модели через кнопку **+ Добавить** (используйте названия скачанных моделей)
   * Например, после `ollama run llama3.2` введите `llama3.2`
   * Используйте **Управление** для редактирования или удаления

## Начало работы

После настройки выберите провайдер **Ollama** и модель в чат-интерфейсе Cherry Studio для взаимодействия с локальной LLM.

## Советы и рекомендации

* **Первоначальная загрузка:** Первый запуск модели требует загрузки файлов — это может занять время.
* **Список моделей:** Команда `ollama list` покажет доступные локальные модели.
* **Системные требования:** Убедитесь, что ваше устройство соответствует аппаратным требованиям выбранной модели (CPU/GPU, память).
* **Документация Ollama:** Перейдите по ссылке `查看Ollama文档和模型` в настройках для доступа к официальной документации.