
{% hint style="warning" %}
Этот документ переведен с китайского языка с помощью ИИ и еще не был проверен.
{% endhint %}

# Пользовательские провайдеры

Cherry Studio не только интегрирует основные сервисы AI-моделей, но и предоставляет вам мощные возможности настройки. С помощью функции **Пользовательские AI-провайдеры** вы можете легко подключить любую необходимую AI-модель.

## Зачем нужны пользовательские AI-провайдеры?

* **Гибкость:** Больше не ограничивайтесь предустановленным списком провайдеров, свободно выбирайте AI-модели, наиболее подходящие вашим потребностям.
* **Разнообразие:** Пробуйте AI-модели с различных платформ, открывая их уникальные преимущества.
* **Управляемость:** Непосредственно контролируйте ваши API-ключи и адреса доступа, обеспечивая безопасность и конфиденциальность.
* **Кастомизация:** Подключайте локально развернутые модели для удовлетворения потребностей конкретных бизнес-сценариев.

## Как добавить пользовательского AI-провайдера?

Всего за несколько простых шагов вы можете добавить своего пользовательского AI-провайдера в Cherry Studio:

<figure><img src="../../.gitbook/assets/image (2) (5).png" alt=""><figcaption></figcaption></figure>

1. **Откройте настройки:** В левой панели навигации Cherry Studio нажмите "Настройки" (значок шестеренки).
2. **Перейдите к моделям:** На странице настроек выберите вкладку "Модели".
3. **Добавьте провайдера:** На странице "Модели" вы увидите список существующих провайдеров. Нажмите кнопку "+ Добавить" внизу списка, чтобы открыть всплывающее окно "Добавить провайдера".
4. **Заполните информацию:** В окне укажите следующие данные:
   * **Название провайдера:** Присвойте вашему кастомному провайдеру узнаваемое имя (например: MyCustomOpenAI).
   * **Тип провайдера:** Выберите тип провайдера из выпадающего списка. Сейчас поддерживаются:
     * OpenAI
     * Gemini
     * Anthropic
     * Azure OpenAI
5. **Сохраните конфигурацию:** После заполнения нажмите кнопку "Добавить", чтобы сохранить ваши настройки.

## Настройка пользовательского AI-провайдера

<figure><img src="../../.gitbook/assets/image (3) (5) (1).png" alt=""><figcaption></figcaption></figure>

После добавления найдите только что добавленного провайдера в списке и выполните детальную настройку:

1. **Статус активности:** В крайней правой колонке списка находится переключатель активности. Включите его для активации сервиса.
2. **API-ключ:**
   * Введите API-ключ, предоставленный вашим AI-провайдером.
   * Нажмите кнопку "Проверить" справа для проверки корректности ключа.
3. **API-адрес:**
   * Укажите базовый URL для доступа к API-сервису.
   * Обязательно сверьтесь с официальной документацией вашего AI-провайдера.
4.  **Управление моделями:**
    * Нажмите "+ Добавить", чтобы вручную добавить ID моделей, которые хотите использовать (например `gpt-3.5-turbo`, `gemini-pro`).

    <figure><img src="../../.gitbook/assets/image (4) (5).png" alt=""><figcaption></figcaption></figure>
    
    * Если вы не уверены в точных названиях моделей, обратитесь к документации провайдера.
    * Кнопка "Управление" позволяет редактировать или удалять добавленные модели.

## Начало работы

После завершения настройки вы можете выбрать вашего кастомного AI-провайдера и модель в чат-интерфейсе Cherry Studio и начать диалог с AI!

## Использование vLLM в качестве пользовательского AI-провайдера

vLLM — это быстрая и удобная библиотека для вывода LLM, похожая на Ollama. Вот как интегрировать vLLM в Cherry Studio:

1.  **Установите vLLM:** Следуйте официальной документации vLLM ([https://docs.vllm.ai/en/latest/getting_started/quickstart.html](https://docs.vllm.ai/en/latest/getting_started/quickstart.html)).

    ```sh
    pip install vllm # при использовании pip
    uv pip install vllm # при использовании uv
    ```
2.  **Запустите сервис vLLM:** Используйте совместимый с OpenAI интерфейс vLLM. Доступны два основных способа:

    * Запуск через `vllm.entrypoints.openai.api_server`

    ```sh
    python -m vllm.entrypoints.openai.api_server --model gpt2
    ```

    * Запуск через `uvicorn`

    ```sh
    vllm --model gpt2 --served-model-name gpt2
    ```

Убедитесь, что сервис запущен и прослушивает порт `8000` (по умолчанию). Вы можете указать другой порт с помощью параметра `--port`.

3. **Добавьте vLLM как провайдера в Cherry Studio:**
   * Следуя предыдущим шагам, добавьте нового кастомного AI-провайдера.
   * **Название провайдера:** `vLLM`
   * **Тип провайдера:** Выберите `OpenAI`.
4. **Настройте провайдера vLLM:**
   * **API-ключ:** vLLM не требует ключа — оставьте поле пустым или введите любое значение.
   * **API-адрес:** Укажите URL vLLM-сервиса. По умолчанию: `http://localhost:8000/` (измените при использовании другого порта).
   * **Управление моделями:** Добавьте имя модели, загруженной в vLLM. Например, для команды `python -m vllm.entrypoints.openai.api_server --model gpt2` укажите `gpt2`.
5. **Начните диалог:** Теперь в Cherry Studio выберите провайдера vLLM и модель `gpt2` для общения с LLM на базе vLLM!

## Советы и рекомендации

* **Внимательно читайте документацию:** Перед добавлением провайдера изучите его официальную документацию по API-ключам, адресам и названиям моделей.
* **Проверяйте API-ключи:** Используйте кнопку "Проверить" для быстрой валидации ключей.
* **Следите за API-адресами:** У разных провайдеров и моделей могут быть разные адреса — всегда указывайте корректный URL.
* **Добавляйте модели по необходимости:** Добавляйте только реально используемые модели, избегая избыточности.