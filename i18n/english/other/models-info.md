
{% hint style="warning" %}
This document was translated from Chinese by AI and has not yet been reviewed.
{% endhint %}

# Common Model Reference Information

{% hint style="info" %}
*   The following information is for reference only. If there are any errors, please contact us for correction. The context size and model information may vary for different providers of some models;
*   When inputting data in the client, "k" needs to be converted to its actual numerical value (theoretically 1k=1024 tokens; 1m=1024k tokens), e.g., 8k is 8×1024=8192 tokens. It is recommended to multiply by 1000 in actual use to prevent errors, e.g., 8k as 8×1000=8000, and 1m as 1×1000000=1000000;
*   A max output of "-" indicates that no clear maximum output information for the model was found from official sources.
{% endhint %}

<table><thead><tr><th width="313">Model Name</th><th width="158">Max Input</th><th width="72">Max Output</th><th width="95">Function Calling</th><th width="142">Model Capabilities</th><th width="540">Provider</th><th width="257">Introduction</th></tr></thead><tbody><tr><td>360gpt-pro</td><td>8k</td><td>-</td><td>Not Supported</td><td>Conversation</td><td>360AI_360gpt</td><td>The flagship hundred-billion-parameter large model in the 360 AI Brain series, with the best performance, widely applicable to complex task scenarios in various fields.</td></tr><tr><td>360gpt-turbo</td><td>7k</td><td>-</td><td>Not Supported</td><td>Conversation</td><td>360AI_360gpt</td><td>A ten-billion-parameter large model that balances performance and effectiveness, suitable for scenarios with high requirements for performance/cost.</td></tr><tr><td>360gpt-turbo-responsibility-8k</td><td>8k</td><td>-</td><td>Not Supported</td><td>Conversation</td><td>360AI_360gpt</td><td>A ten-billion-parameter large model that balances performance and effectiveness, suitable for scenarios with high requirements for performance/cost.</td></tr><tr><td>360gpt2-pro</td><td>8k</td><td>-</td><td>Not Supported</td><td>Conversation</td><td>360AI_360gpt</td><td>The flagship hundred-billion-parameter large model in the 360 AI Brain series, with the best performance, widely applicable to complex task scenarios in various fields.</td></tr><tr><td>claude-3-5-sonnet-20240620</td><td>200k</td><td>16k</td><td>Not Supported</td><td>Conversation, Vision</td><td>Anthropic_claude</td><td>A snapshot version released on June 20, 2024. Claude 3.5 Sonnet is a model that balances performance and speed, offering top-tier performance while maintaining high speed, and supports multimodal input.</td></tr><tr><td>claude-3-5-haiku-20241022</td><td>200k</td><td>16k</td><td>Not Supported</td><td>Conversation</td><td>Anthropic_claude</td><td>A snapshot version released on October 22, 2024. Claude 3.5 Haiku has improved across various skills, including coding, tool use, and reasoning. As the fastest model in the Anthropic family, it provides rapid response times, suitable for applications requiring high interactivity and low latency, such as user-facing chatbots and instant code completion. It also excels in specialized tasks like data extraction and real-time content moderation, making it a versatile tool for wide application across industries. It does not support image input.</td></tr><tr><td>claude-3-5-sonnet-20241022</td><td>200k</td><td>8K</td><td>Not Supported</td><td>Conversation, Vision</td><td>Anthropic_claude</td><td>A snapshot version released on October 22, 2024. Claude 3.5 Sonnet offers capabilities surpassing Opus and faster speeds than Sonnet, while maintaining the same price as Sonnet. Sonnet is particularly adept at programming, data science, visual processing, and agentic tasks.</td></tr><tr><td>claude-3-5-sonnet-latest</td><td>200K</td><td>8k</td><td>Not Supported</td><td>Conversation, Vision</td><td>Anthropic_claude</td><td>Dynamically points to the latest Claude 3.5 Sonnet version. Claude 3.5 Sonnet offers capabilities surpassing Opus and faster speeds than Sonnet, while maintaining the same price as Sonnet. Sonnet is particularly adept at programming, data science, visual processing, and agentic tasks. This model points to the latest version.</td></tr><tr><td>claude-3-haiku-20240307</td><td>200k</td><td>4k</td><td>Not Supported</td><td>Conversation, Vision</td><td>Anthropic_claude</td><td>Claude 3 Haiku is Anthropic's fastest and most compact model, designed for near-instantaneous responses. It features fast and accurate targeted performance.</td></tr><tr><td>claude-3-opus-20240229</td><td>200k</td><td>4k</td><td>Not Supported</td><td>Conversation, Vision</td><td>Anthropic_claude</td><td>Claude 3 Opus is Anthropic's most powerful model for handling highly complex tasks. It excels in performance, intelligence, fluency, and comprehension.</td></tr><tr><td>claude-3-sonnet-20240229</td><td>200k</td><td>8k</td><td>Not Supported</td><td>Conversation, Vision</td><td>Anthropic_claude</td><td>A snapshot version released on February 29, 2024. Sonnet is particularly adept at:<br><br>- Coding: Can autonomously write, edit, and run code, with reasoning and troubleshooting capabilities<br>- Data Science: Enhances human data science expertise; can process unstructured data when using multiple tools to gain insights<br>- Visual Processing: Excels at interpreting charts, graphs, and images, accurately transcribing text to extract insights beyond the text itself<br>- Agentic Tasks: Excellent tool use, making it ideal for handling agentic tasks (i.e., complex, multi-step problem-solving that requires interaction with other systems)</td></tr><tr><td>google/gemma-2-27b-it</td><td>8k</td><td>-</td><td>Not Supported</td><td>Conversation</td><td>Google_gamma</td><td>Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models. They are decoder-only large language models that support English and come with open weights, pre-trained, and instruction-tuned variants. Gemma models are well-suited for a variety of text generation tasks, including question answering, summarization, and reasoning.</td></tr><tr><td>google/gemma-2-9b-it</td><td>8k</td><td>-</td><td>Not Supported</td><td>Conversation</td><td>Google_gamma</td><td>Gemma is one of the lightweight, state-of-the-art open model series developed by Google. It is a decoder-only large language model that supports English, with open weights, pre-trained, and instruction-tuned variants available. Gemma models are suitable for various text generation tasks, including question answering, summarization, and reasoning. This 9B model was trained on 8 trillion tokens.</td></tr><tr><td>gemini-1.5-pro</td><td>2m</td><td>8k</td><td>Not Supported</td><td>Conversation</td><td>Google_gemini</td><td>The latest stable version of Gemini 1.5 Pro. As a powerful multimodal model, it can handle up to 60,000 lines of code or 2,000 pages of text. It is particularly suitable for tasks requiring complex reasoning.</td></tr><tr><td>gemini-1.0-pro-001</td><td>33k</td><td>8k</td><td>Not Supported</td><td>Conversation</td><td>Google_gemini</td><td>This is a stable version of Gemini 1.0 Pro. As an NLP model, it specializes in tasks like multi-turn text and code chat, as well as code generation. This model will be discontinued on February 15, 2025, and it is recommended to migrate to the 1.5 series models.</td></tr><tr><td>gemini-1.0-pro-002</td><td>32k</td><td>8k</td><td>Not Supported</td><td>Conversation</td><td>Google_gemini</td><td>This is a stable version of Gemini 1.0 Pro. As an NLP model, it specializes in tasks like multi-turn text and code chat, as well as code generation. This model will be discontinued on February 15, 2025, and it is recommended to migrate to the 1.5 series models.</td></tr><tr><td>gemini-1.0-pro-latest</td><td>33k</td><td>8k</td><td>Not Supported</td><td>Conversation, Deprecated or soon to be deprecated</td><td>Google_gemini</td><td>This is the latest version of Gemini 1.0 Pro. As an NLP model, it specializes in tasks like multi-turn text and code chat, as well as code generation. This model will be discontinued on February 15, 2025, and it is recommended to migrate to the 1.5 series models.</td></tr><tr><td>gemini-1.0-pro-vision-001</td><td>16k</td><td>2k</td><td>Not Supported</td><td>Conversation</td><td>Google_gemini</td><td>This is the vision version of Gemini 1.0 Pro. This model will be discontinued on February 15, 2025, and it is recommended to migrate to the 1.5 series models.</td></tr><tr><td>gemini-1.0-pro-vision-latest</td><td>16k</td><td>2k</td><td>Not Supported</td><td>Vision</td><td>Google_gemini</td><td>This is the latest vision version of Gemini 1.0 Pro. This model will be discontinued on February 15, 2025, and it is recommended to migrate to the 1.5 series models.</td></tr><tr><td>gemini-1.5-flash</td><td>1m</td><td>8k</td><td>Not Supported</td><td>Conversation, Vision</td><td>Google_gemini</td><td>This is the latest stable version of Gemini 1.5 Flash. As a balanced multimodal model, it can process audio, image, video, and text inputs.</td></tr><tr><td>gemini-1.5-flash-001</td><td>1m</td><td>8k</td><td>Not Supported</td><td>Conversation, Vision</td><td>Google_gemini</td><td>This is a stable version of Gemini 1.5 Flash. It offers the same basic features as gemini-1.5-flash but is version-pinned, making it suitable for production environments.</td></tr><tr><td>gemini-1.5-flash-002</td><td>1m</td><td>8k</td><td>Not Supported</td><td>Conversation, Vision</td><td>Google_gemini</td><td>This is a stable version of Gemini 1.5 Flash. It offers the same basic features as gemini-1.5-flash but is version-pinned, making it suitable for production environments.</td></tr><tr><td>gemini-1.5-flash-8b</td><td>1m</td><td>8k</td><td>Not Supported</td><td>Conversation, Vision</td><td>Google_gemini</td><td>Gemini 1.5 Flash-8B is Google's latest multimodal AI model, designed for efficient handling of large-scale tasks. With 8 billion parameters, the model supports text, image, audio, and video inputs, making it suitable for various application scenarios such as chat, transcription, and translation. Compared to other Gemini models, Flash-8B is optimized for speed and cost-effectiveness, especially for cost-sensitive users. Its rate limit is doubled, allowing developers to handle large-scale tasks more efficiently. Additionally, Flash-8B uses "knowledge distillation" technology to extract key knowledge from larger models, ensuring it is lightweight and efficient while retaining core capabilities.</td></tr><tr><td>gemini-1.5-flash-exp-0827</td><td>1m</td><td>8k</td><td>Not Supported</td><td>Conversation, Vision</td><td>Google_gemini</td><td>This is an experimental version of Gemini 1.5 Flash, which is regularly updated with the latest improvements. It is suitable for exploratory testing and prototyping, but not recommended for production environments.</td></tr><tr><td>gemini-1.5-flash-latest</td><td>1m</td><td>8k</td><td>Not Supported</td><td>Conversation, Vision</td><td>Google_gemini</td><td>This is the cutting-edge version of Gemini 1.5 Flash, which is regularly updated with the latest improvements. It is suitable for exploratory testing and prototyping, but not recommended for production environments.</td></tr><tr><td>gemini-1.5-pro-001</td><td>2m</td><td>8k</td><td>Not Supported</td><td>Conversation, Vision</td><td>Google_gemini</td><td>This is a stable version of Gemini 1.5 Pro, offering fixed model behavior and performance characteristics. It is suitable for production environments that require stability.</td></tr><tr><td>gemini-1.5-pro-002</td><td>2m</td><td>8k</td><td>Not Supported</td><td>Conversation, Vision</td><td>Google_gemini</td><td>This is a stable version of Gemini 1.5 Pro, offering fixed model behavior and performance characteristics. It is suitable for production environments that require stability.</td></tr><tr><td>gemini-1.5-pro-exp-0801</td><td>2m</td><td>8k</td><td>Not Supported</td><td>Conversation, Vision</td><td>Google_gemini</td><td>An experimental version of Gemini 1.5 Pro. As a powerful multimodal model, it can handle up to 60,000 lines of code or 2,000 pages of text. It is particularly suitable for tasks requiring complex reasoning.</td></tr><tr><td>gemini-1.5-pro-exp-0827</td><td>2m</td><td>8k</td><td>Not Supported</td><td>Conversation, Vision</td><td>Google_gemini</td><td>An experimental version of Gemini 1.5 Pro. As a powerful multimodal model, it can handle up to 60,000 lines of code or 2,000 pages of text. It is particularly suitable for tasks requiring complex reasoning.</td></tr><tr><td>gemini-1.5-pro-latest</td><td>2m</td><td>8k</td><td>Not Supported</td><td>Conversation, Vision</td><td>Google_gemini</td><td>This is the latest version of Gemini 1.5 Pro, dynamically pointing to the most recent snapshot version.</td></tr><tr><td>gemini-2.0-flash</td><td>1m</td><td>8k</td><td>Not Supported</td><td>Conversation, Vision</td><td>Google_gemini</td><td>Gemini 2.0 Flash is Google's latest model, featuring a faster Time to First Token (TTFT) compared to the 1.5 version, while maintaining a quality level comparable to Gemini Pro 1.5. This model shows significant improvements in multimodal understanding, coding ability, complex instruction following, and function calling, thereby providing a smoother and more powerful intelligent experience.</td></tr><tr><td>gemini-2.0-flash-exp</td><td>100k</td><td>8k</td><td>Supported</td><td>Conversation, Vision</td><td>Google_gemini</td><td>Gemini 2.0 Flash introduces a real-time multimodal API, improved speed and performance, enhanced quality, stronger agent capabilities, and adds image generation and voice conversion functions.</td></tr><tr><td>gemini-2.0-flash-lite-preview-02-05</td><td>1M</td><td>8k</td><td>Not Supported</td><td>Conversation, Vision</td><td>Google_gemini</td><td>Gemini 2.0 Flash-Lite is Google's latest cost-effective AI model, offering better quality at the same speed as 1.5 Flash. It supports a 1 million token context window and can handle multimodal tasks involving images, audio, and code. As Google's most cost-effective model currently, it uses a simplified single pricing strategy, making it particularly suitable for large-scale application scenarios that require cost control.</td></tr><tr><td>gemini-2.0-flash-thinking-exp</td><td>40k</td><td>8k</td><td>Not Supported</td><td>Conversation, Reasoning</td><td>Google_gemini</td><td>gemini-2.0-flash-thinking-exp is an experimental model that can generate the "thinking process" it goes through when formulating a response. Therefore, "thinking mode" responses have stronger reasoning capabilities compared to the basic Gemini 2.0 Flash model.</td></tr><tr><td>gemini-2.0-flash-thinking-exp-01-21</td><td>1m</td><td>64k</td><td>Not Supported</td><td>Conversation, Reasoning</td><td>Google_gemini</td><td>Gemini 2.0 Flash Thinking EXP-01-21 is Google's latest AI model, focusing on enhancing reasoning abilities and user interaction experience. The model has strong reasoning capabilities, especially in math and programming, and supports a context window of up to 1 million tokens, suitable for complex tasks and in-depth analysis scenarios. Its unique feature is the ability to generate its thinking process, improving the comprehensibility of AI thinking. It also supports native code execution, enhancing the flexibility and practicality of interactions. By optimizing algorithms, the model reduces logical contradictions, further improving the accuracy and consistency of its answers.</td></tr><tr><td>gemini-2.0-flash-thinking-exp-1219</td><td>40k</td><td>8k</td><td>Not Supported</td><td>Conversation, Reasoning, Vision</td><td>Google_gemini</td><td>gemini-2.0-flash-thinking-exp-1219 is an experimental model that can generate the "thinking process" it goes through when formulating a response. Therefore, "thinking mode" responses have stronger reasoning capabilities compared to the basic Gemini 2.0 Flash model.</td></tr><tr><td>gemini-2.0-pro-exp-01-28</td><td>2m</td><td>64k</td><td>Not Supported</td><td>Conversation, Vision</td><td>Google_gemini</td><td>Pre-announced model, not yet online.</td></tr><tr><td>gemini-2.0-pro-exp-02-05</td><td>2m</td><td>8k</td><td>Not Supported</td><td>Conversation, Vision</td><td>Google_gemini</td><td>Gemini 2.0 Pro Exp 02-05 is Google's latest experimental model released in February 2024, excelling in world knowledge, code generation, and long-text understanding. The model supports an ultra-long context window of 2 million tokens, capable of processing content equivalent to 2 hours of video, 22 hours of audio, over 60,000 lines of code, and more than 1.4 million words. As part of the Gemini 2.0 series, this model adopts a new Flash Thinking training strategy, significantly improving its performance and ranking high on several LLM leaderboards, demonstrating strong comprehensive capabilities.</td></tr><tr><td>gemini-exp-1114</td><td>8k</td><td>4k</td><td>Not Supported</td><td>Conversation, Vision</td><td>Google_gemini</td><td>This is an experimental model released on November 14, 2024, primarily focusing on quality improvements.</td></tr><tr><td>gemini-exp-1121</td><td>8k</td><td>4k</td><td>Not Supported</td><td>Conversation, Vision, Code</td><td>Google_gemini</td><td>This is an experimental model released on November 21, 2024, with improvements in coding, reasoning, and visual capabilities.</td></tr><tr><td>gemini-exp-1206</td><td>8k</td><td>4k</td><td>Not Supported</td><td>Conversation, Vision</td><td>Google_gemini</td><td>This is an experimental model released on December 6, 2024, with improvements in coding, reasoning, and visual capabilities.</td></tr><tr><td>gemini-exp-latest</td><td>8k</td><td>4k</td><td>Not Supported</td><td>Conversation, Vision</td><td>Google_gemini</td><td>This is an experimental model, dynamically pointing to the latest version.</td></tr><tr><td>gemini-pro</td><td>33k</td><td>8k</td><td>Not Supported</td><td>Conversation</td><td>Google_gemini</td><td>Same as gemini-1.0-pro, it is an alias for gemini-1.0-pro.</td></tr><tr><td>gemini-pro-vision</td><td>16k</td><td>2k</td><td>Not Supported</td><td>Conversation, Vision</td><td>Google_gemini</td><td>This is the vision version of Gemini 1.0 Pro. This model will be discontinued on February 15, 2025, and it is recommended to migrate to the 1.5 series models.</td></tr><tr><td>grok-2</td><td>128k</td><td>-</td><td>Not Supported</td><td>Conversation</td><td>Grok_grok</td><td>A new version of the grok model released by X.ai on December 12, 2024.</td></tr><tr><td>grok-2-1212</td><td>128k</td><td>-</td><td>Not Supported</td><td>Conversation</td><td>Grok_grok</td><td>A new version of the grok model released by X.ai on December 12, 2024.</td></tr><tr><td>grok-2-latest</td><td>128k</td><td>-</td><td>Not Supported</td><td>Conversation</td><td>Grok_grok</td><td>A new version of the grok model released by X.ai on December 12, 2024.</td></tr><tr><td>grok-2-vision-1212</td><td>32k</td><td>-</td><td>Not Supported</td><td>Conversation, Vision</td><td>Grok_grok</td><td>The grok vision version model released by X.ai on December 12, 2024.</td></tr><tr><td>grok-beta</td><td>100k</td><td>-</td><td>Not Supported</td><td>Conversation</td><td>Grok_grok</td><td>Performance comparable to Grok 2, but with improved efficiency, speed, and functionality.</td></tr><tr><td>grok-vision-beta</td><td>8k</td><td>-</td><td>Not Supported</td><td>Conversation, Vision</td><td>Grok_grok</td><td>The latest image understanding model can process various visual information, including documents, charts, screenshots, and photos.</td></tr><tr><td>internlm/internlm2_5-20b-chat</td><td>32k</td><td>-</td><td>Supported</td><td>Conversation</td><td>internlm</td><td>InternLM2.5-20B-Chat is an open-source large-scale conversational model developed based on the InternLM2 architecture. With 20 billion parameters, this model excels in mathematical reasoning, surpassing comparable models like Llama3 and Gemma2-27B. InternLM2.5-20B-Chat has significantly improved tool-calling capabilities, supporting information collection from hundreds of web pages for analysis and reasoning, and possessing stronger instruction understanding, tool selection, and result reflection abilities.</td></tr><tr><td>meta-llama/Llama-3.2-11B-Vision-Instruct</td><td>8k</td><td>-</td><td>Not Supported</td><td>Conversation, Vision</td><td>Meta_llama</td><td>The current Llama series models can not only process text data but also image data. Some models in Llama 3.2 have added visual understanding functions. This model supports simultaneous input of text and image data, understands the image, and outputs text information.</td></tr><tr><td>meta-llama/Llama-3.2-3B-Instruct</td><td>32k</td><td>-</td><td>Not Supported</td><td>Conversation</td><td>Meta_llama</td><td>Meta Llama 3.2 multilingual Large Language Models (LLMs), where 1B and 3B are lightweight models that can run on edge and mobile devices. This model is the 3B version.</td></tr><tr><td>meta-llama/Llama-3.2-90B-Vision-Instruct</td><td>8k</td><td>-</td><td>Not Supported</td><td>Conversation, Vision</td><td>Meta_llama</td><td>The current Llama series models can not only process text data but also image data. Some models in Llama 3.2 have added visual understanding functions. This model supports simultaneous input of text and image data, understands the image, and outputs text information.</td></tr><tr><td>meta-llama/Llama-3.3-70B-Instruct</td><td>131k</td><td>-</td><td>Not Supported</td><td>Conversation</td><td>Meta_llama</td><td>Meta's latest 70B LLM, with performance comparable to Llama 3.1 405B.</td></tr><tr><td>meta-llama/Meta-Llama-3.1-405B-Instruct</td><td>32k</td><td>-</td><td>Not Supported</td><td>Conversation</td><td>Meta_llama</td><td>The Meta Llama 3.1 multilingual Large Language Model (LLM) collection is a set of pre-trained and instruction-tuned generative models in 8B, 70B, and 405B sizes. This model is the 405B version. The Llama 3.1 instruction-tuned text models (8B, 70B, 405B) are optimized for multilingual conversations and outperform many available open-source and closed-source chat models on common industry benchmarks.</td></tr><tr><td>meta-llama/Meta-Llama-3.1-70B-Instruct</td><td>32k</td><td>-</td><td>Not Supported</td><td>Conversation</td><td>Meta_llama</td><td>Meta Llama 3.1 is a family of multilingual large language models developed by Meta, including pre-trained and instruction-tuned variants in 8B, 70B, and 405B parameter sizes. This 70B instruction-tuned model is optimized for multilingual conversation scenarios and performs excellently on several industry benchmarks. The model was trained on over 15 trillion tokens of public data and uses techniques like supervised fine-tuning and reinforcement learning with human feedback to enhance its usefulness and safety.</td></tr><tr><td>meta-llama/Meta-Llama-3.1-8B-Instruct</td><td>32k</td><td>-</td><td>Not Supported</td><td>Conversation</td><td>Meta_llama</td><td>The Meta Llama 3.1 multilingual Large Language Model (LLM) collection is a set of pre-trained and instruction-tuned generative models in 8B, 70B, and 405B sizes. This model is the 8B version. The Llama 3.1 instruction-tuned text models (8B, 70B, 405B) are optimized for multilingual conversations and outperform many available open-source and closed-source chat models on common industry benchmarks.</td></tr><tr><td>abab5.5-chat</td><td>16k</td><td>-</td><td>Supported</td><td>Conversation</td><td>Minimax_abab</td><td>Chinese persona conversation scenarios.</td></tr><tr><td>abab5.5s-chat</td><td>8k</td><td>-</td><td>Supported</td><td>Conversation</td><td>Minimax_abab</td><td>Chinese persona conversation scenarios.</td></tr><tr><td>abab6.5g-chat</td><td>8k</td><td>-</td><td>Supported</td><td>Conversation</td><td>Minimax_abab</td><td>Persona conversation scenarios in English and other languages.</td></tr><tr><td>abab6.5s-chat</td><td>245k</td><td>-</td><td>Supported</td><td>Conversation</td><td>Minimax_abab</td><td>General scenarios.</td></tr><tr><td>abab6.5t-chat</td><td>8k</td><td>-</td><td>Supported</td><td>Conversation</td><td>Minimax_abab</td><td>Chinese persona conversation scenarios.</td></tr><tr><td>chatgpt-4o-latest</td><td>128k</td><td>16k</td><td>Not Supported</td><td>Conversation, Vision</td><td>OpenAI</td><td>The chatgpt-4o-latest model version continuously points to the GPT-4o version used in ChatGPT and is updated the fastest when there are significant changes.</td></tr><tr><td>gpt-4o-2024-11-20</td><td>128k</td><td>16k</td><td>Supported</td><td>Conversation</td><td>OpenAI</td><td>The latest gpt-4o snapshot version from November 20, 2024.</td></tr><tr><td>gpt-4o-audio-preview</td><td>128k</td><td>16k</td><td>Not Supported</td><td>Conversation</td><td>OpenAI</td><td>OpenAI's real-time voice conversation model.</td></tr><tr><td>gpt-4o-audio-preview-2024-10-01</td><td>128k</td><td>16k</td><td>Supported</td><td>Conversation</td><td>OpenAI</td><td>OpenAI's real-time voice conversation model.</td></tr><tr><td>o1</td><td>128k</td><td>32k</td><td>Not Supported</td><td>Conversation, Reasoning, Vision</td><td>OpenAI</td><td>OpenAI's new reasoning model for complex tasks that require extensive common sense. The model has a 200k context, is currently the most powerful model in the world, and supports image recognition.</td></tr><tr><td>o1-mini-2024-09-12</td><td>128k</td><td>64k</td><td>Not Supported</td><td>Conversation, Reasoning</td><td>OpenAI</td><td>A fixed snapshot version of o1-mini. It is smaller, faster, and 80% cheaper than o1-preview, performing well in code generation and small-context operations.</td></tr><tr><td>o1-preview-2024-09-12</td><td>128k</td><td>32k</td><td>Not Supported</td><td>Conversation, Reasoning</td><td>OpenAI</td><td>A fixed snapshot version of o1-preview.</td></tr><tr><td>gpt-3.5-turbo</td><td>16k</td><td>4k</td><td>Supported</td><td>Conversation</td><td>OpenAI_gpt-3</td><td>Based on GPT-3.5: GPT-3.5 Turbo is an improved version built on the GPT-3.5 model, developed by OpenAI.<br>Performance Goals: Designed to improve model inference speed, processing efficiency, and resource utilization through optimized model structure and algorithms.<br>Increased Inference Speed: Compared to GPT-3.5, GPT-3.5 Turbo typically offers faster inference speeds on the same hardware, which is particularly beneficial for applications requiring large-scale text processing.<br>Higher Throughput: When processing a large number of requests or data, GPT-3.5 Turbo can achieve higher concurrent processing capabilities, thereby increasing overall system throughput.<br>Optimized Resource Consumption: While maintaining performance, it may have reduced demand for hardware resources (such as memory and computing resources), which helps lower operating costs and improve system scalability.<br>Wide Range of NLP Tasks: GPT-3.5 Turbo is suitable for a variety of natural language processing tasks, including but not limited to text generation, semantic understanding, dialogue systems, and machine translation.<br>Developer Tools and API Support: Provides API interfaces that are easy for developers to integrate and use, supporting rapid application development and deployment.</td></tr><tr><td>gpt-3.5-turbo-0125</td><td>16k</td><td>4k</td><td>Supported</td><td>Conversation</td><td>OpenAI_gpt-3</td><td>An updated GPT 3.5 Turbo model with higher accuracy in responding to requested formats and a fix for a bug that caused text encoding issues for non-English language function calls. Returns a maximum of 4,096 output tokens.</td></tr><tr><td>gpt-3.5-turbo-0613</td><td>16k</td><td>4k</td><td>Supported</td><td>Conversation</td><td>OpenAI_gpt-3</td><td>Updated fixed snapshot version of GPT 3.5 Turbo. Now deprecated.</td></tr><tr><td>gpt-3.5-turbo-1106</td><td>16k</td><td>4k</td><td>Supported</td><td>Conversation</td><td>OpenAI_gpt-3</td><td>Features improved instruction following, JSON mode, reproducible outputs, parallel function calling, and more. Returns a maximum of 4,096 output tokens.</td></tr><tr><td>gpt-3.5-turbo-16k</td><td>16k</td><td>4k</td><td>Supported</td><td>Conversation, Deprecated or soon to be deprecated</td><td>OpenAI_gpt-3</td><td>(Deprecated)</td></tr><tr><td>gpt-3.5-turbo-16k-0613</td><td>16k</td><td>4k</td><td>Supported</td><td>Conversation, Deprecated or soon to be deprecated</td><td>OpenAI_gpt-3</td><td>A snapshot of gpt-3.5-turbo from June 13, 2023. (Deprecated)</td></tr><tr><td>gpt-3.5-turbo-instruct</td><td>4k</td><td>4k</td><td>Supported</td><td>Conversation</td><td>OpenAI_gpt-3</td><td>Capabilities similar to GPT-3 era models. Compatible with the legacy Completions endpoint, not for Chat Completions.</td></tr><tr><td>gpt-3.5o</td><td>16k</td><td>4k</td><td>Not Supported</td><td>Conversation</td><td>OpenAI_gpt-3</td><td>Same as gpt-4o-lite.</td></tr><tr><td>gpt-4</td><td>8k</td><td>8k</td><td>Supported</td><td>Conversation</td><td>OpenAI_gpt-4</td><td>Currently points to gpt-4-0613.</td></tr><tr><td>gpt-4-0125-preview</td><td>128k</td><td>4k</td><td>Supported</td><td>Conversation</td><td>OpenAI_gpt-4</td><td>The latest GPT-4 model, designed to reduce "laziness" where the model does not complete tasks. Returns a maximum of 4,096 output tokens.</td></tr><tr><td>gpt-4-0314</td><td>8k</td><td>8k</td><td>Supported</td><td>Conversation</td><td>OpenAI_gpt-4</td><td>A snapshot of gpt-4 from March 14, 2023.</td></tr><tr><td>gpt-4-0613</td><td>8k</td><td>8k</td><td>Supported</td><td>Conversation</td><td>OpenAI_gpt-4</td><td>A snapshot of gpt-4 from June 13, 2023, with enhanced function calling support.</td></tr><tr><td>gpt-4-1106-preview</td><td>128k</td><td>4k</td><td>Supported</td><td>Conversation</td><td>OpenAI_gpt-4</td><td>A GPT-4 Turbo model with improved instruction following, JSON mode, reproducible outputs, function calling, and more. Returns a maximum of 4,096 output tokens. This is a preview model.</td></tr><tr><td>gpt-4-32k</td><td>32k</td><td>4k</td><td>Supported</td><td>Conversation</td><td>OpenAI_gpt-4</td><td>gpt-4-32k will be deprecated on 2025-06-06.</td></tr><tr><td>gpt-4-32k-0613</td><td>32k</td><td>4k</td><td>Supported</td><td>Conversation, Deprecated or soon to be deprecated</td><td>OpenAI_gpt-4</td><td>Will be deprecated on 2025-06-06.</td></tr><tr><td>gpt-4-turbo</td><td>128k</td><td>4k</td><td>Supported</td><td>Conversation</td><td>OpenAI_gpt-4</td><td>The latest version of the GPT-4 Turbo model adds vision capabilities, supporting visual requests via JSON mode and function calling. The current version of this model is gpt-4-turbo-2024-04-09.</td></tr><tr><td>gpt-4-turbo-2024-04-09</td><td>128k</td><td>4k</td><td>Supported</td><td>Conversation</td><td>OpenAI_gpt-4</td><td>GPT-4 Turbo model with vision capabilities. Vision requests can now be made via JSON mode and function calling. gpt-4-turbo currently points to this version.</td></tr><tr><td>gpt-4-turbo-preview</td><td>128k</td><td>4k</td><td>Supported</td><td>Conversation, Vision</td><td>OpenAI_gpt-4</td><td>Currently points to gpt-4-0125-preview.</td></tr><tr><td>gpt-4o</td><td>128k</td><td>16k</td><td>Supported</td><td>Conversation, Vision</td><td>OpenAI_gpt-4</td><td>OpenAI's highly intelligent flagship model, suitable for complex, multi-step tasks. GPT-4o is cheaper and faster than GPT-4 Turbo.</td></tr><tr><td>gpt-4o-2024-05-13</td><td>128k</td><td>4k</td><td>Supported</td><td>Conversation, Vision</td><td>OpenAI_gpt-4</td><td>The original gpt-4o snapshot from May 13, 2024.</td></tr><tr><td>gpt-4o-2024-08-06</td><td>128k</td><td>16k</td><td>Supported</td><td>Conversation, Vision</td><td>OpenAI_gpt-4</td><td>The first snapshot to support structured outputs. gpt-4o currently points to this version.</td></tr><tr><td>gpt-4o-mini</td><td>128k</td><td>16k</td><td>Supported</td><td>Conversation, Vision</td><td>OpenAI_gpt-4</td><td>OpenAI's affordable version of gpt-4o, suitable for fast, lightweight tasks. GPT-4o mini is cheaper and more powerful than GPT-3.5 Turbo. Currently points to gpt-4o-mini-2024-07-18.</td></tr><tr><td>gpt-4o-mini-2024-07-18</td><td>128k</td><td>16k</td><td>Supported</td><td>Conversation, Vision</td><td>OpenAI_gpt-4</td><td>A fixed snapshot version of gpt-4o-mini.</td></tr><tr><td>gpt-4o-realtime-preview</td><td>128k</td><td>4k</td><td>Supported</td><td>Conversation, Real-time Voice</td><td>OpenAI_gpt-4</td><td>OpenAI's real-time voice conversation model.</td></tr><tr><td>gpt-4o-realtime-preview-2024-10-01</td><td>128k</td><td>4k</td><td>Supported</td><td>Conversation, Real-time Voice, Vision</td><td>OpenAI_gpt-4</td><td>gpt-4o-realtime-preview currently points to this snapshot version.</td></tr><tr><td>o1-mini</td><td>128k</td><td>64k</td><td>Not Supported</td><td>Conversation, Reasoning</td><td>OpenAI_o1</td><td>Smaller, faster, and 80% cheaper than o1-preview, performing well in code generation and small-context operations.</td></tr><tr><td>o1-preview</td><td>128k</td><td>32k</td><td>Not Supported</td><td>Conversation, Reasoning</td><td>OpenAI_o1</td><td>o1-preview is a new reasoning model for complex tasks that require extensive common sense. The model has a 128K context and a knowledge cutoff of October 2023. It focuses on advanced reasoning and solving complex problems, including mathematical and scientific tasks. It is ideal for applications requiring deep contextual understanding and autonomous workflows.</td></tr><tr><td>o3-mini</td><td>200k</td><td>100k</td><td>Supported</td><td>Conversation, Reasoning</td><td>OpenAI_o1</td><td>o3-mini is OpenAI's latest small reasoning model, offering high intelligence while maintaining the same cost and latency as o1-mini. It focuses on science, math, and coding tasks, supports developer features like structured output, function calling, and batch API, with a knowledge cutoff of October 2023, demonstrating a significant balance in reasoning capability and cost-effectiveness.</td></tr><tr><td>o3-mini-2025-01-31</td><td>200k</td><td>100k</td><td>Supported</td><td>Conversation, Reasoning</td><td>OpenAI_o1</td><td>o3-mini currently points to this version. o3-mini-2025-01-31 is OpenAI's latest small reasoning model, offering high intelligence while maintaining the same cost and latency as o1-mini. It focuses on science, math, and coding tasks, supports developer features like structured output, function calling, and batch API, with a knowledge cutoff of October 2023, demonstrating a significant balance in reasoning capability and cost-effectiveness.</td></tr><tr><td>Baichuan2-Turbo</td><td>32k</td><td>-</td><td>Not Supported</td><td>Conversation</td><td>Baichuan_baichuan</td><td>Compared to similarly sized models in the industry, this model maintains a leading performance while significantly reducing the price.</td></tr><tr><td>Baichuan3-Turbo</td><td>32k</td><td>-</td><td>Not Supported</td><td>Conversation</td><td>Baichuan_baichuan</td><td>Compared to similarly sized models in the industry, this model maintains a leading performance while significantly reducing the price.</td></tr><tr><td>Baichuan3-Turbo-128k</td><td>128k</td><td>-</td><td>Not Supported</td><td>Conversation</td><td>Baichuan_baichuan</td><td>The Baichuan model processes complex text with a 128k ultra-long context window, is specifically optimized for industries like finance, and significantly reduces costs while maintaining high performance, providing a cost-effective solution for enterprises.</td></tr><tr><td>Baichuan4</td><td>32k</td><td>-</td><td>Not Supported</td><td>Conversation</td><td>Baichuan_baichuan</td><td>Baichuan's MoE model provides a highly efficient and cost-effective solution for enterprise applications through specialized optimization, cost reduction, and performance enhancement.</td></tr><tr><td>Baichuan4-Air</td><td>32k</td><td>-</td><td>Not Supported</td><td>Conversation</td><td>Baichuan_baichuan</td><td>Baichuan's MoE model provides a highly efficient and cost-effective solution for enterprise applications through specialized optimization, cost reduction, and performance enhancement.</td></tr><tr><td>Baichuan4-Turbo</td><td>32k</td><td>-</td><td>Not Supported</td><td>Conversation</td><td>Baichuan_baichuan</td><td>Trained on massive high-quality scenario data, usability in high-frequency enterprise scenarios is improved by 10%+ compared to Baichuan4, information summarization by 50%, multilingual capabilities by 31%, and content generation by 13%.<br>Specially optimized for inference performance, the first token response speed is increased by 51% and token stream speed by 73% compared to Baichuan4.</td></tr><tr><td>ERNIE-3.5-128K</td><td>128k</td><td>4k</td><td>Supported</td><td>Conversation</td><td>Baidu_ernie</td><td>Baidu's self-developed flagship large language model, covering massive Chinese and English corpora, with powerful general capabilities to meet most dialogue, Q&A, creative generation, and plugin application requirements. Supports automatic integration with the Baidu search plugin to ensure the timeliness of Q&A information.</td></tr><tr><td>ERNIE-3.5-8K</td><td>8k</td><td>1k</td><td>Supported</td><td>Conversation</td><td>Baidu_ernie</td><td>Baidu's self-developed flagship large language model, covering massive Chinese and English corpora, with powerful general capabilities to meet most dialogue, Q&A, creative generation, and plugin application requirements. Supports automatic integration with the Baidu search plugin to ensure the timeliness of Q&A information.</td></tr><tr><td>ERNIE-3.5-8K-Preview</td><td>8k</td><td>1k</td><td>Supported</td><td>Conversation</td><td>Baidu_ernie</td><td>Baidu's self-developed flagship large language model, covering massive Chinese and English corpora, with powerful general capabilities to meet most dialogue, Q&A, creative generation, and plugin application requirements. Supports automatic integration with the Baidu search plugin to ensure the timeliness of Q&A information.</td></tr><tr><td>ERNIE-4.0-8K</td><td>8k</td><td>1k</td><td>Supported</td><td>Conversation</td><td>Baidu_ernie</td><td>Baidu's self-developed flagship ultra-large-scale language model. Compared to ERNIE 3.5, it has a comprehensive upgrade in model capabilities, widely applicable to complex task scenarios in various fields. Supports automatic integration with the Baidu search plugin to ensure the timeliness of Q&A information.</td></tr><tr><td>ERNIE-4.0-8K-Latest</td><td>8k</td><td>2k</td><td>Supported</td><td>Conversation</td><td>Baidu_ernie</td><td>ERNIE-4.0-8K-Latest has fully improved capabilities compared to ERNIE-4.0-8K, with significant enhancements in role-playing and instruction-following abilities. Compared to ERNIE 3.5, it has a comprehensive upgrade in model capabilities, widely applicable to complex task scenarios in various fields. Supports automatic integration with the Baidu search plugin to ensure the timeliness of Q&A information, and supports 5K tokens input + 2K tokens output. This document introduces the method for calling the ERNIE-4.0-8K-Latest API.</td></tr><tr><td>ERNIE-4.0-8K-Preview</td><td>8k</td><td>1k</td><td>Supported</td><td>Conversation</td><td>Baidu_ernie</td><td>Baidu's self-developed flagship ultra-large-scale language model. Compared to ERNIE 3.5, it has a comprehensive upgrade in model capabilities, widely applicable to complex task scenarios in various fields. Supports automatic integration with the Baidu search plugin to ensure the timeliness of Q&A information.</td></tr><tr><td>ERNIE-4.0-Turbo-128K</td><td>128k</td><td>4k</td><td>Supported</td><td>Conversation</td><td>Baidu_ernie</td><td>ERNIE 4.0 Turbo is Baidu's self-developed flagship ultra-large-scale language model with outstanding overall performance, widely applicable to complex task scenarios in various fields. Supports automatic integration with the Baidu search plugin to ensure the timeliness of Q&A information. It has better performance compared to ERNIE 4.0. ERNIE-4.0-Turbo-128K is a version of the model with better overall performance on long documents than ERNIE-3.5-128K. This document introduces the relevant API and its usage.</td></tr><tr><td>ERNIE-4.0-Turbo-8K</td><td>8k</td><td>2k</td><td>Supported</td><td>Conversation</td><td>Baidu_ernie</td><td>ERNIE 4.0 Turbo is Baidu's self-developed flagship ultra-large-scale language model with outstanding overall performance, widely applicable to complex task scenarios in various fields. Supports automatic integration with the Baidu search plugin to ensure the timeliness of Q&A information. It has better performance compared to ERNIE 4.0. ERNIE-4.0-Turbo-8K is a version of the model. This document introduces the relevant API and its usage.</td></tr><tr><td>ERNIE-4.0-Turbo-8K-Latest</td><td>8k</td><td>2k</td><td>Supported</td><td>Conversation</td><td>Baidu_ernie</td><td>ERNIE 4.0 Turbo is Baidu's self-developed flagship ultra-large-scale language model with outstanding overall performance, widely applicable to complex task scenarios in various fields. Supports automatic integration with the Baidu search plugin to ensure the timeliness of Q&A information. It has better performance compared to ERNIE 4.0. ERNIE-4.0-Turbo-8K is a version of the model.</td></tr><tr><td>ERNIE-4.0-Turbo-8K-Preview</td><td>8k</td><td>2k</td><td>Supported</td><td>Conversation</td><td>Baidu_ernie</td><td>ERNIE 4.0 Turbo is Baidu's self-developed flagship ultra-large-scale language model with outstanding overall performance, widely applicable to complex task scenarios in various fields. Supports automatic integration with the Baidu search plugin to ensure the timeliness of Q&A information. ERNIE-4.0-Turbo-8K-Preview is a version of the model.</td></tr><tr><td>ERNIE-Character-8K</td><td>8k</td><td>1k</td><td>Not Supported</td><td>Conversation</td><td>Baidu_ernie</td><td>Baidu's self-developed vertical large language model, suitable for application scenarios such as game NPCs, customer service dialogues, and dialogue role-playing. It has a more distinct and consistent persona style, stronger instruction-following ability, and better inference performance.</td></tr><tr><td>ERNIE-Lite-8K</td><td>8k</td><td>4k</td><td>Not Supported</td><td>Conversation</td><td>Baidu_ernie</td><td>Baidu's self-developed lightweight large language model, balancing excellent model performance with inference efficiency, suitable for inference on low-power AI accelerator cards.</td></tr><tr><td>ERNIE-Lite-Pro-128K</td><td>128k</td><td>2k</td><td>Supported</td><td>Conversation</td><td>Baidu_ernie</td><td>Baidu's self-developed lightweight large language model, with better performance than ERNIE Lite, balancing excellent model performance with inference efficiency, suitable for inference on low-power AI accelerator cards. ERNIE-Lite-Pro-128K supports a 128K context length and has better performance than ERNIE-Lite-128K.</td></tr><tr><td>ERNIE-Novel-8K</td><td>8k</td><td>2k</td><td>Not Supported</td><td>Conversation</td><td>Baidu_ernie</td><td>ERNIE-Novel-8K is Baidu's self-developed general-purpose large language model, with a significant advantage in novel continuation capabilities. It can also be used in scenarios like short dramas and movies.</td></tr><tr><td>ERNIE-Speed-128K</td><td>128k</td><td>4k</td><td>Not Supported</td><td>Conversation</td><td>Baidu_ernie</td><td>Baidu's latest self-developed high-performance large language model released in 2024, with excellent general capabilities. It is suitable as a base model for fine-tuning to better handle specific scenario problems, while also having excellent inference performance.</td></tr><tr><td>ERNIE-Speed-8K</td><td>8k</td><td>1k</td><td>Not Supported</td><td>Conversation</td><td>Baidu_ernie</td><td>Baidu's latest self-developed high-performance large language model released in 2024, with excellent general capabilities. It is suitable as a base model for fine-tuning to better handle specific scenario problems, while also having excellent inference performance.</td></tr><tr><td>ERNIE-Speed-Pro-128K</td><td>128k</td><td>4k</td><td>Not Supported</td><td>Conversation</td><td>Baidu_ernie</td><td>ERNIE Speed Pro is Baidu's latest self-developed high-performance large language model released in 2024, with excellent general capabilities. It is suitable as a base model for fine-tuning to better handle specific scenario problems, while also having excellent inference performance. ERNIE-Speed-Pro-128K is the initial version released on August 30, 2024, supporting a 128K context length and having better performance than ERNIE-Speed-128K.</td></tr><tr><td>ERNIE-Tiny-8K</td><td>8k</td><td>1k</td><td>Not Supported</td><td>Conversation</td><td>Baidu_ernie</td><td>Baidu's self-developed ultra-high-performance large language model, with the lowest deployment and fine-tuning costs in the ERNIE series.</td></tr><tr><td>Doubao-1.5-lite-32k</td><td>32k</td><td>12k</td><td>Supported</td><td>Conversation</td><td>Doubao_doubao</td><td>Doubao1.5-lite is also among the world's top-tier lightweight language models, matching or surpassing GPT-4o mini and Claude 3.5 Haiku on authoritative evaluation benchmarks for general knowledge (MMLU_pro), reasoning (BBH), math (MATH), and professional knowledge (GPQA).<br></td></tr><tr><td>Doubao-1.5-pro-256k</td><td>256k</td><td>12k</td><td>Supported</td><td>Conversation</td><td>Doubao_doubao</td><td>Doubao-1.5-Pro-256k, a fully upgraded version based on Doubao-1.5-Pro. Compared to Doubao-pro-256k/241115, the overall performance is significantly improved by 10%. The output length is greatly increased, supporting up to 12k tokens.</td></tr><tr><td>Doubao-1.5-pro-32k</td><td>32k</td><td>12k</td><td>Supported</td><td>Conversation</td><td>Doubao_doubao</td><td>Doubao-1.5-pro, a new generation flagship model with comprehensive performance upgrades, excelling in knowledge, code, reasoning, and more. It achieves world-leading performance on multiple public evaluation benchmarks, especially achieving the best scores on knowledge, code, reasoning, and Chinese authoritative benchmarks, with a composite score superior to top industry models like GPT4o and Claude 3.5 Sonnet.</td></tr><tr><td>Doubao-1.5-vision-pro</td><td>32k</td><td>12k</td><td>Not Supported</td><td>Conversation, Vision</td><td>Doubao_doubao</td><td>Doubao-1.5-vision-pro, a newly upgraded multimodal large model, supports image recognition of any resolution and extreme aspect ratios, enhancing visual reasoning, document recognition, detailed information understanding, and instruction-following capabilities.</td></tr><tr><td>Doubao-embedding</td><td>4k</td><td>-</td><td>Supported</td><td>Embedding</td><td>Doubao_doubao</td><td>Doubao-embedding is a semantic vectorization model developed by ByteDance, primarily for vector retrieval scenarios. It supports Chinese and English, with a maximum context length of 4K. The following versions are currently available:<br><br>text-240715: Maximum vector dimension of 2560, supports dimensionality reduction to 512, 1024, and 2048. Chinese and English retrieval performance is significantly improved compared to the text-240515 version, and this version is recommended.<br>text-240515: Maximum vector dimension of 2048, supports dimensionality reduction to 512 and 1024.</td></tr><tr><td>Doubao-embedding-large</td><td>4k</td><td>-</td><td>Not Supported</td><td>Embedding</td><td>Doubao_doubao</td><td><br>Chinese and English retrieval performance is significantly improved compared to the Doubao-embedding/text-240715 version.</td></tr><tr><td>Doubao-embedding-vision</td><td>8k</td><td>-</td><td>Not Supported</td><td>Embedding</td><td>Doubao_doubao</td><td>Doubao-embedding-vision, a newly upgraded image-text multimodal vectorization model, is primarily for image-text multi-vector retrieval scenarios. It supports image input and Chinese/English text input, with a maximum context length of 8K.</td></tr><tr><td>Doubao-lite-128k</td><td>128k</td><td>4k</td><td>Supported</td><td>Conversation</td><td>Doubao_doubao</td><td>Doubao-lite offers extremely fast response speeds and better cost-effectiveness, providing more flexible choices for customers in different scenarios. Supports inference and fine-tuning with a 128k context window.</td></tr><tr><td>Doubao-lite-32k</td><td>32k</td><td>4k</td><td>Supported</td><td>Conversation</td><td>Doubao_doubao</td><td>Doubao-lite offers extremely fast response speeds and better cost-effectiveness, providing more flexible choices for customers in different scenarios. Supports inference and fine-tuning with a 32k context window.</td></tr><tr><td>Doubao-lite-4k</td><td>4k</td><td>4k</td><td>Supported</td><td>Conversation</td><td>Doubao_doubao</td><td>Doubao-lite offers extremely fast response speeds and better cost-effectiveness, providing more flexible choices for customers in different scenarios. Supports inference and fine-tuning with a 4k context window.</td></tr><tr><td>Doubao-pro-128k</td><td>128k</td><td>4k</td><td>Supported</td><td>Conversation</td><td>Doubao_doubao</td><td>The flagship model with the best performance, suitable for handling complex tasks, with excellent results in reference Q&A, summarization, creation, text classification, role-playing, and other scenarios. Supports inference and fine-tuning with a 128k context window.</td></tr><tr><td>Doubao-pro-32k</td><td>32k</td><td>4k</td><td>Supported</td><td>Conversation</td><td>Doubao_doubao</td><td>The flagship model with the best performance, suitable for handling complex tasks, with excellent results in reference Q&A, summarization, creation, text classification, role-playing, and other scenarios. Supports inference and fine-tuning with a 32k context window.</td></tr><tr><td>Doubao-pro-4k</td><td>4k</td><td>4k</td><td>Supported</td><td>Conversation</td><td>Doubao_doubao</td><td>The flagship model with the best performance, suitable for handling complex tasks, with excellent results in reference Q&A, summarization, creation, text classification, role-playing, and other scenarios. Supports inference and fine-tuning with a 4k context window.</td></tr><tr><td>step-1-128k</td><td>128k</td><td>-</td><td>Supported</td><td>Conversation</td><td>StepFun</td><td>The step-1-128k model is an ultra-large-scale language model capable of processing inputs of up to 128,000 tokens. This capability gives it a significant advantage in generating long-form content and performing complex reasoning, making it suitable for applications that require rich context, such as writing novels and scripts.</td></tr><tr><td>step-1-256k</td><td>256k</td><td>-</td><td>Supported</td><td>Conversation</td><td>StepFun</td><td>The step-1-256k model is one of the largest language models available, supporting inputs of 256,000 tokens. It is designed to meet extremely complex task requirements, such as large-scale data analysis and multi-turn dialogue systems, and can provide high-quality output in various domains.</td></tr><tr><td>step-1-32k</td><td>32k</td><td>-</td><td>Supported</td><td>Conversation</td><td>StepFun</td><td>The step-1-32k model extends the context window to support 32,000 tokens of input. This makes it perform excellently when handling long articles and complex conversations, suitable for tasks that require deep understanding and analysis, such as legal documents and academic research.</td></tr><tr><td>step-1-8k</td><td>8k</td><td>-</td><td>Supported</td><td>Conversation</td><td>StepFun</td><td>The step-1-8k model is an efficient language model designed for processing shorter texts. It can perform reasoning within a context of 8,000 tokens, making it suitable for application scenarios that require quick responses, such as chatbots and real-time translation.</td></tr><tr><td>step-1-flash</td><td>8k</td><td>-</td><td>Supported</td><td>Conversation</td><td>StepFun</td><td>The step-1-flash model focuses on rapid response and efficient processing, suitable for real-time applications. Its design allows it to provide high-quality language understanding and generation capabilities even with limited computing resources, making it suitable for mobile devices and edge computing scenarios.</td></tr><tr><td>step-1.5v-mini</td><td>32k</td><td>-</td><td>Supported</td><td>Conversation, Vision</td><td>StepFun</td><td>The step-1.5v-mini model is a lightweight version designed to run in resource-constrained environments. Despite its small size, it still retains good language processing capabilities, making it suitable for embedded systems and low-power devices.</td></tr><tr><td>step-1v-32k</td><td>32k</td><td>-</td><td>Supported</td><td>Conversation, Vision</td><td>StepFun</td><td>The step-1v-32k model supports inputs of 32,000 tokens, suitable for applications requiring longer context. It performs excellently in handling complex dialogues and long texts, making it suitable for fields such as customer service and content creation.</td></tr><tr><td>step-1v-8k</td><td>8k</td><td>-</td><td>Supported</td><td>Conversation, Vision</td><td>StepFun</td><td>The step-1v-8k model is an optimized version designed for 8,000-token inputs, suitable for fast generation and processing of short texts. It strikes a good balance between speed and accuracy, making it suitable for real-time applications.</td></tr><tr><td>step-2-16k</td><td>16k</td><td>-</td><td>Supported</td><td>Conversation</td><td>StepFun</td><td>The step-2-16k model is a medium-sized language model supporting 16,000 tokens of input. It performs well in various tasks and is suitable for application scenarios such as education, training, and knowledge management.<br></td></tr><tr><td>yi-lightning</td><td>16k</td><td>-</td><td>Supported</td><td>Conversation</td><td>01.AI_yi</td><td>The latest high-performance model, ensuring high-quality output while significantly increasing inference speed.<br>Suitable for real-time interaction and highly complex reasoning scenarios, its extremely high cost-effectiveness can provide excellent support for commercial products.</td></tr><tr><td>yi-vision-v2</td><td>16K</td><td>-</td><td>Supported</td><td>Conversation, Vision</td><td>01.AI_yi</td><td>Suitable for scenarios that require analyzing and interpreting images and charts, such as image Q&A, chart understanding, OCR, visual reasoning, education, research report understanding, or multilingual document reading.</td></tr><tr><td>qwen-14b-chat</td><td>8k</td><td>2k</td><td>Supported</td><td>Conversation</td><td>Qwen_qwen</td><td>Alibaba Cloud's official open-source version of Tongyi Qianwen.</td></tr><tr><td>qwen-72b-chat</td><td>32k</td><td>2k</td><td>Supported</td><td>Conversation</td><td>Qwen_qwen</td><td>Alibaba Cloud's official open-source version of Tongyi Qianwen.</td></tr><tr><td>qwen-7b-chat</td><td>7.5k</td><td>1.5k</td><td>Supported</td><td>Conversation</td><td>Qwen_qwen</td><td>Alibaba Cloud's official open-source version of Tongyi Qianwen.</td></tr><tr><td>qwen-coder-plus</td><td>128k</td><td>8k</td><td>Supported</td><td>Conversation, Code</td><td>Qwen_qwen</td><td>Qwen-Coder-Plus is a programming-specific model in the Qwen series, designed to enhance code generation and understanding capabilities. Trained on a large scale of programming data, this model can handle multiple programming languages and supports functions like code completion, error detection, and code refactoring. Its design goal is to provide developers with more efficient programming assistance and improve development efficiency.</td></tr><tr><td>qwen-coder-plus-latest</td><td>128k</td><td>8k</td><td>Supported</td><td>Conversation, Code</td><td>Qwen_qwen</td><td>Qwen-Coder-Plus-Latest is the newest version of Qwen-Coder-Plus, incorporating the latest algorithm optimizations and dataset updates. This model shows significant performance improvements, enabling it to understand context more accurately and generate code that better meets developers' needs. It also introduces support for more programming languages, enhancing its multilingual programming capabilities.</td></tr><tr><td>qwen-coder-turbo</td><td>128k</td><td>8k</td><td>Supported</td><td>Conversation, Code</td><td>Qwen_qwen</td><td>The Tongyi Qianwen series of code and programming models are language models specifically for programming and code generation, featuring fast inference speed and low cost. This version always points to the latest stable snapshot.</td></tr><tr><td>qwen-coder-turbo-latest</td><td>128k</td><td>8k</td><td>Supported</td><td>Conversation, Code</td><td>Qwen_qwen</td><td>The Tongyi Qianwen series of code and programming models are language models specifically for programming and code generation, featuring fast inference speed and low cost. This version always points to the latest snapshot.</td></tr><tr><td>qwen-long</td><td>10m</td><td>6k</td><td>Supported</td><td>Conversation</td><td>Qwen_qwen</td><td>Qwen-Long is a large language model from Tongyi Qianwen for ultra-long context processing scenarios. It supports input in different languages such as Chinese and English, and supports ultra-long context dialogues of up to 10 million tokens (about 15 million words or 15,000 pages of documents). Combined with the synchronously launched document service, it can parse and have dialogues on various document formats such as Word, PDF, Markdown, EPUB, and MOBI. Note: For requests submitted directly via HTTP, it supports a length of 1M tokens. For lengths exceeding this, it is recommended to submit via file.</td></tr><tr><td>qwen-math-plus</td><td>4k</td><td>3k</td><td>Supported</td><td>Conversation</td><td>Qwen_qwen</td><td>Qwen-Math-Plus is a model focused on solving mathematical problems, designed to provide efficient mathematical reasoning and calculation capabilities. Trained on a large number of math problems, this model can handle complex mathematical expressions and problems, supporting a variety of calculation needs from basic arithmetic to higher mathematics. Its application scenarios include education, scientific research, and engineering.</td></tr><tr><td>qwen-math-plus-latest</td><td>4k</td><td>3k</td><td>Supported</td><td>Conversation</td><td>Qwen_qwen</td><td>Qwen-Math-Plus-Latest is the newest version of Qwen-Math-Plus, integrating the latest mathematical reasoning techniques and algorithm improvements. This model performs better in handling complex mathematical problems, providing more accurate solutions and reasoning processes. It also expands its understanding of mathematical symbols and formulas, making it suitable for a wider range of mathematical applications.</td></tr><tr><td>qwen-math-turbo</td><td>4k</td><td>3k</td><td>Supported</td><td>Conversation</td><td>Qwen_qwen</td><td>Qwen-Math-Turbo is a high-performance mathematical model designed for fast calculation and real-time inference. This model optimizes calculation speed, enabling it to process a large number of mathematical problems in a very short time, suitable for application scenarios that require quick feedback, such as online education and real-time data analysis. Its efficient algorithms allow users to get instant results in complex calculations.</td></tr><tr><td>qwen-math-turbo-latest</td><td>4k</td><td>3k</td><td>Supported</td><td>Conversation</td><td>Qwen_qwen</td><td>Qwen-Math-Turbo-Latest is the newest version of Qwen-Math-Turbo, further improving calculation efficiency and accuracy. This model has undergone multiple algorithmic optimizations, enabling it to handle more complex mathematical problems and maintain high efficiency in real-time inference. It is suitable for mathematical applications that require rapid response, such as financial analysis and scientific computing.</td></tr><tr><td>qwen-max</td><td>32k</td><td>8k</td><td>Supported</td><td>Conversation</td><td>Qwen_qwen</td><td>The Tongyi Qianwen 2.5 series hundred-billion-level ultra-large-scale language model supports input in different languages such as Chinese and English. As the model is upgraded, qwen-max will be updated on a rolling basis.</td></tr><tr><td>qwen-max-latest</td><td>32k</td><td>8k</td><td>Supported</td><td>Conversation</td><td>Qwen_qwen</td><td>The best-performing model in the Tongyi Qianwen series. This model is a dynamically updated version, and model updates will not be announced in advance. It is suitable for complex, multi-step tasks. The model's comprehensive abilities in Chinese and English are significantly improved, human preference is significantly enhanced, reasoning ability and complex instruction understanding are significantly strengthened, performance on difficult tasks is better, and math and code abilities are significantly improved. It also has enhanced understanding and generation capabilities for structured data like tables and JSON.</td></tr><tr><td>qwen-plus</td><td>128k</td><td>8k</td><td>Supported</td><td>Conversation</td><td>Qwen_qwen</td><td>A well-balanced model in the Tongyi Qianwen series, with inference performance and speed between Tongyi Qianwen-Max and Tongyi Qianwen-Turbo, suitable for moderately complex tasks. The model's comprehensive abilities in Chinese and English are significantly improved, human preference is significantly enhanced, reasoning ability and complex instruction understanding are significantly strengthened, performance on difficult tasks is better, and math and code abilities are significantly improved.</td></tr><tr><td>qwen-plus-latest</td><td>128k</td><td>8k</td><td>Supported</td><td>Conversation</td><td>Qwen_qwen</td><td>Qwen-Plus is an enhanced version of the visual language model in the Tongyi Qianwen series, designed to improve detail recognition and text recognition capabilities. This model supports images with resolutions over one million pixels and any aspect ratio, performing excellently in a wide range of visual language tasks, making it suitable for applications requiring high-precision image understanding.</td></tr><tr><td>qwen-turbo</td><td>128k</td><td>8k</td><td>Supported</td><td>Conversation</td><td>Qwen_qwen</td><td>The fastest and most cost-effective model in the Tongyi Qianwen series, suitable for simple tasks. The model's comprehensive abilities in Chinese and English are significantly improved, human preference is significantly enhanced, reasoning ability and complex instruction understanding are significantly strengthened, performance on difficult tasks is better, and math and code abilities are significantly improved.</td></tr><tr><td>qwen-turbo-latest</td><td>1m</td><td>8k</td><td>Supported</td><td>Conversation</td><td>Qwen_qwen</td><td>Qwen-Turbo is an efficient model designed for simple tasks, emphasizing speed and cost-effectiveness. It performs excellently in basic visual language tasks and is suitable for applications with strict response time requirements, such as real-time image recognition and simple Q&A systems.</td></tr><tr><td>qwen-vl-max</td><td>32k</td><td>2k</td><td>Supported</td><td>Conversation</td><td>Qwen_qwen</td><td>Tongyi Qianwen VL-Max (qwen-vl-max), the ultra-large-scale visual language model from Tongyi Qianwen. Compared to the enhanced version, it further improves visual reasoning and instruction-following capabilities, providing a higher level of visual perception and cognition. It offers the best performance on more complex tasks.</td></tr><tr><td>qwen-vl-max-latest</td><td>32k</td><td>2k</td><td>Supported</td><td>Conversation, Vision</td><td>Qwen_qwen</td><td>Qwen-VL-Max is the most advanced version in the Qwen-VL series, designed to solve complex multimodal tasks. It combines advanced visual and language processing technologies, capable of understanding and analyzing high-resolution images with extremely strong reasoning abilities, suitable for applications requiring deep understanding and complex reasoning.</td></tr><tr><td>qwen-vl-ocr</td><td>34k</td><td>4k</td><td>Supported</td><td>Conversation, Vision</td><td>Qwen_qwen</td><td>Only supports OCR, not conversation.</td></tr><tr><td>qwen-vl-ocr-latest</td><td>34k</td><td>4k</td><td>Supported</td><td>Conversation, Vision</td><td>Qwen_qwen</td><td>Only supports OCR, not conversation.</td></tr><tr><td>qwen-vl-plus</td><td>8k</td><td>2k</td><td>Supported</td><td>Conversation, Vision</td><td>Qwen_qwen</td><td>Tongyi Qianwen VL-Plus (qwen-vl-plus), the enhanced version of the Tongyi Qianwen large-scale visual language model. It significantly improves detail recognition and text recognition capabilities, supports images with resolutions over one million pixels and any aspect ratio. It provides excellent performance on a wide range of visual tasks.</td></tr><tr><td>qwen-vl-plus-latest</td><td>32k</td><td>2k</td><td>Supported</td><td>Conversation, Vision</td><td>Qwen_qwen</td><td>Qwen-VL-Plus-Latest is the newest version of Qwen-VL-Plus, enhancing the model's multimodal understanding capabilities. It excels in the combined processing of images and text, making it suitable for applications that need to efficiently handle multiple input formats, such as intelligent customer service and content generation.</td></tr><tr><td>Qwen/Qwen2-1.5B-Instruct</td><td>32k</td><td>6k</td><td>Not Supported</td><td>Conversation</td><td>Qwen_qwen</td><td>Qwen2-1.5B-Instruct is an instruction-tuned large language model in the Qwen2 series with a parameter size of 1.5B. Based on the Transformer architecture, the model uses SwiGLU activation functions, attention QKV biases, and group query attention. It excels in multiple benchmark tests for language understanding, generation, multilingual capabilities, coding, math, and reasoning, surpassing most open-source models.</td></tr><tr><td>Qwen/Qwen2-72B-Instruct</td><td>128k</td><td>6k</td><td>Not Supported</td><td>Conversation</td><td>Qwen_qwen</td><td>Qwen2-72B-Instruct is an instruction-tuned large language model in the Qwen2 series with a parameter size of 72B. Based on the Transformer architecture, the model uses SwiGLU activation functions, attention QKV biases, and group query attention. It can handle large-scale inputs. The model excels in multiple benchmark tests for language understanding, generation, multilingual capabilities, coding, math, and reasoning, surpassing most open-source models.</td></tr><tr><td>Qwen/Qwen2-7B-Instruct</td><td>128k</td><td>6k</td><td>Not Supported</td><td>Conversation</td><td>Qwen_qwen</td><td>Qwen2-7B-Instruct is an instruction-tuned large language model in the Qwen2 series with a parameter size of 7B. Based on the Transformer architecture, the model uses SwiGLU activation functions, attention QKV biases, and group query attention. It can handle large-scale inputs. The model excels in multiple benchmark tests for language understanding, generation, multilingual capabilities, coding, math, and reasoning, surpassing most open-source models.</td></tr><tr><td>Qwen/Qwen2-VL-72B-Instruct</td><td>32k</td><td>2k</td><td>Not Supported</td><td>Conversation</td><td>Qwen_qwen</td><td>Qwen2-VL is the latest iteration of the Qwen-VL model, achieving state-of-the-art performance in visual understanding benchmarks, including MathVista, DocVQA, RealWorldQA, and MTVQA. Qwen2-VL can understand videos over 20 minutes long for high-quality video-based Q&A, dialogue, and content creation. It also has complex reasoning and decision-making capabilities, and can be integrated with mobile devices, robots, etc., for automated operations based on visual environments and text instructions.</td></tr><tr><td>Qwen/Qwen2-VL-7B-Instruct</td><td>32k</td><td>-</td><td>Not Supported</td><td>Conversation</td><td>Qwen_qwen</td><td>Qwen2-VL-7B-Instruct is the latest iteration of the Qwen-VL model, achieving state-of-the-art performance in visual understanding benchmarks, including MathVista, DocVQA, RealWorldQA, and MTVQA. Qwen2-VL can be used for high-quality video-based Q&A, dialogue, and content creation, and also has complex reasoning and decision-making capabilities, and can be integrated with mobile devices, robots, etc., for automated operations based on visual environments and text instructions.</td></tr><tr><td>Qwen/Qwen2.5-72B-Instruct</td><td>128k</td><td>8k</td><td>Not Supported</td><td>Conversation</td><td>Qwen_qwen</td><td>Qwen2.5-72B-Instruct is one of the latest large language model series released by Alibaba Cloud. This 72B model has significantly improved capabilities in areas such as coding and mathematics. It supports inputs of up to 128K tokens and can generate long texts of over 8K tokens.</td></tr><tr><td>Qwen/Qwen2.5-72B-Instruct-128K</td><td>128k</td><td>8k</td><td>Not Supported</td><td>Conversation</td><td>Qwen_qwen</td><td>Qwen2.5-72B-Instruct is one of the latest large language model series released by Alibaba Cloud. This 72B model has significantly improved capabilities in areas such as coding and mathematics. It supports inputs of up to 128K tokens and can generate long texts of over 8K tokens.</td></tr><tr><td>Qwen/Qwen2.5-7B-Instruct</td><td>128k</td><td>8k</td><td>Not Supported</td><td>Conversation</td><td>Qwen_qwen</td><td>Qwen2.5-7B-Instruct is one of the latest large language model series released by Alibaba Cloud. This 7B model has significantly improved capabilities in areas such as coding and mathematics. The model also provides multilingual support, covering over 29 languages, including Chinese and English. The model has significant improvements in instruction following, understanding structured data, and generating structured output (especially JSON).</td></tr><tr><td>Qwen/Qwen2.5-Coder-32B-Instruct</td><td>128k</td><td>8k</td><td>Not Supported</td><td>Conversation, Code</td><td>Qwen_qwen</td><td>Qwen2.5-32B-Instruct is one of the latest large language model series released by Alibaba Cloud. This 32B model has significantly improved capabilities in areas such as coding and mathematics. The model also provides multilingual support, covering over 29 languages, including Chinese and English. The model has significant improvements in instruction following, understanding structured data, and generating structured output (especially JSON).</td></tr><tr><td>Qwen/Qwen2.5-Coder-7B-Instruct</td><td>128k</td><td>8k</td><td>Not Supported</td><td>Conversation</td><td>Qwen_qwen</td><td>Qwen2.5-7B-Instruct is one of the latest large language model series released by Alibaba Cloud. This 7B model has significantly improved capabilities in areas such as coding and mathematics. The model also provides multilingual support, covering over 29 languages, including Chinese and English. The model has significant improvements in instruction following, understanding structured data, and generating structured output (especially JSON).</td></tr><tr><td>Qwen/QwQ-32B-Preview</td><td>32k</td><td>16k</td><td>Not Supported</td><td>Conversation, Reasoning</td><td>Qwen_qwen</td><td>QwQ-32B-Preview is an experimental research model developed by the Qwen team, aimed at enhancing the reasoning capabilities of artificial intelligence. As a preview version, it demonstrates excellent analytical abilities, but also has some important limitations:<br>1. Language mixing and code-switching: The model may mix languages or switch between languages unexpectedly, affecting the clarity of the response.<br>2. Recursive reasoning loops: The model may enter a cyclic reasoning mode, leading to lengthy answers without a clear conclusion.<br>3. Safety and ethical considerations: The model requires strengthened safety measures to ensure reliable and safe performance, and users should exercise caution when using it.<br>4. Performance and benchmark limitations: The model performs excellently in mathematics and programming, but there is still room for improvement in other areas such as common sense reasoning and nuanced language understanding.</td></tr><tr><td>qwen1.5-110b-chat</td><td>32k</td><td>8k</td><td>Not Supported</td><td>Conversation</td><td>Qwen_qwen</td><td>-</td></tr><tr><td>qwen1.5-14b-chat</td><td>8k</td><td>2k</td><td>Not Supported</td><td>Conversation</td><td>Qwen_qwen</td><td>-</td></tr><tr><td>qwen1.5-32b-chat</td><td>32k</td><td>2k</td><td>Not Supported</td><td>Conversation</td><td>Qwen_qwen</td><td>-</td></tr><tr><td>qwen1.5-72b-chat</td><td>32k</td><td>2k</td><td>Not Supported</td><td>Conversation</td><td>Qwen_qwen</td><td>-</td></tr><tr><td>qwen1.5-7b-chat</td><td>8k</td><td>2k</td><td>Not Supported</td><td>Conversation</td><td>Qwen_qwen</td><td>-</td></tr><tr><td>qwen2-57b-a14b-instruct</td><td>65k</td><td>6k</td><td>Not Supported</td><td>Conversation</td><td>Qwen_qwen</td><td>-</td></tr><tr><td>Qwen2-72B-Instruct</td><td>-</td><td>-</td><td>Not Supported</td><td>Conversation</td><td>Qwen_qwen</td><td>-</td></tr><tr><td>qwen2-7b-instruct</td><td>128k</td><td>6k</td><td>Not Supported</td><td>Conversation</td><td>Qwen_qwen</td><td>-</td></tr><tr><td>qwen2-math-72b-instruct</td><td>4k</td><td>3k</td><td>Not Supported</td><td>Conversation</td><td>Qwen_qwen</td><td>-</td></tr><tr><td>qwen2-math-7b-instruct</td><td>4k</td><td>3k</td><td>Not Supported</td><td>Conversation</td><td>Qwen_qwen</td><td>-</td></tr><tr><td>qwen2.5-14b-instruct</td><td>128k</td><td>8k</td><td>Not Supported</td><td>Conversation</td><td>Qwen_qwen</td><td>-</td></tr><tr><td>qwen2.5-32b-instruct</td><td>128k</td><td>8k</td><td>Not Supported</td><td>Conversation</td><td>Qwen_qwen</td><td>-</td></tr><tr><td>qwen2.5-72b-instruct</td><td>128k</td><td>8k</td><td>Not Supported</td><td>Conversation</td><td>Qwen_qwen</td><td>-</td></tr><tr><td>qwen2.5-7b-instruct</td><td>128k</td><td>8k</td><td>Not Supported</td><td>Conversation</td><td>Qwen_qwen</td><td>-</td></tr><tr><td>qwen2.5-coder-14b-instruct</td><td>128k</td><td>8k</td><td>Not Supported</td><td>Conversation, Code</td><td>Qwen_qwen</td><td>-</td></tr><tr><td>qwen2.5-coder-32b-instruct</td><td>128k</td><td>8k</td><td>Not Supported</td><td>Conversation, Code</td><td>Qwen_qwen</td><td>-</td></tr><tr><td>qwen2.5-coder-7b-instruct</td><td>128k</td><td>8k</td><td>Not Supported</td><td>Conversation, Code</td><td>Qwen_qwen</td><td>-</td></tr><tr><td>qwen2.5-math-72b-instruct</td><td>4k</td><td>3k</td><td>Not Supported</td><td>Conversation</td><td>Qwen_qwen</td><td>-</td></tr><tr><td>qwen2.5-math-7b-instruct</td><td>4k</td><td>3k</td><td>Not Supported</td><td>Conversation</td><td>Qwen_qwen</td><td>-</td></tr><tr><td>deepseek-ai/DeepSeek-R1</td><td>64k</td><td>-</td><td>Not Supported</td><td>Conversation, Reasoning</td><td>DeepSeek_deepseek</td><td>The DeepSeek-R1 model is an open-source reasoning model based purely on reinforcement learning. It excels in tasks such as mathematics, code, and natural language reasoning, with performance comparable to OpenAI's o1 model and achieving excellent results in several benchmark tests.</td></tr><tr><td>deepseek-ai/DeepSeek-V2-Chat</td><td>128k</td><td>-</td><td>Not Supported</td><td>Conversation</td><td>DeepSeek_deepseek</td><td>DeepSeek-V2 is a powerful, cost-effective Mixture-of-Experts (MoE) language model. It was pre-trained on a high-quality corpus of 8.1 trillion tokens and further enhanced with Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL). Compared to DeepSeek 67B, DeepSeek-V2 achieves stronger performance while saving 42.5% in training costs, reducing KV cache by 93.3%, and increasing maximum generation throughput by 5.76 times.</td></tr><tr><td>deepseek-ai/DeepSeek-V2.5</td><td>32k</td><td>-</td><td>Supported</td><td>Conversation</td><td>DeepSeek_deepseek</td><td>DeepSeek-V2.5 is an upgraded version of DeepSeek-V2-Chat and DeepSeek-Coder-V2-Instruct, integrating the general and coding capabilities of the two previous versions. This model has been optimized in several aspects, including writing and instruction-following abilities, to better align with human preferences.</td></tr><tr><td>deepseek-ai/DeepSeek-V3</td><td>128k</td><td>4k</td><td>Not Supported</td><td>Conversation</td><td>DeepSeek_deepseek</td><td>Open-source version of deepseek. Compared to the official version, it has a longer context and no issues with sensitive word refusal.</td></tr><tr><td>deepseek-chat</td><td>64k</td><td>8k</td><td>Supported</td><td>Conversation</td><td>DeepSeek_deepseek</td><td>236B parameters, 64K context (API), top-ranked on the open-source leaderboard for Chinese comprehensive ability (AlignBench), and in the same tier as closed-source models like GPT-4-Turbo and ERNIE 4.0 in evaluations.</td></tr><tr><td>deepseek-coder</td><td>64k</td><td>8k</td><td>Supported</td><td>Conversation, Code</td><td>DeepSeek_deepseek</td><td>236B parameters, 64K context (API), top-ranked on the open-source leaderboard for Chinese comprehensive ability (AlignBench), and in the same tier as closed-source models like GPT-4-Turbo and ERNIE 4.0 in evaluations.</td></tr><tr><td>deepseek-reasoner</td><td>64k</td><td>8k</td><td>Supported</td><td>Conversation, Reasoning</td><td>DeepSeek_deepseek</td><td>DeepSeek-Reasoner (DeepSeek-R1) is the latest reasoning model from DeepSeek, designed to enhance reasoning capabilities through reinforcement learning training. The model's reasoning process involves a large amount of reflection and validation, enabling it to handle complex logical reasoning tasks, with a chain-of-thought length that can reach tens of thousands of words. DeepSeek-R1 excels in solving mathematical, coding, and other complex problems and has been widely applied in various scenarios, demonstrating its powerful reasoning ability and flexibility. Compared to other models, DeepSeek-R1's reasoning performance is close to that of top-tier closed-source models, showcasing the potential and competitiveness of open-source models in the field of reasoning.</td></tr><tr><td>hunyuan-code</td><td>4k</td><td>4k</td><td>Not Supported</td><td>Conversation, Code</td><td>Tencent_hunyuan</td><td>Hunyuan's latest code generation model. The base model was augmented with 200B high-quality code data and trained with high-quality SFT data for half a year. The context window length has been increased to 8K. It ranks at the top in automatic evaluation metrics for code generation in five major languages. In high-quality manual evaluations of 10 comprehensive code tasks across five major languages, its performance is in the top tier.</td></tr><tr><td>hunyuan-functioncall</td><td>28k</td><td>4k</td><td>Supported</td><td>Conversation</td><td>Tencent_hunyuan</td><td>Hunyuan's latest MOE architecture FunctionCall model, trained with high-quality FunctionCall data, with a context window of up to 32K, leading in evaluation metrics across multiple dimensions.</td></tr><tr><td>hunyuan-large</td><td>28k</td><td>4k</td><td>Not Supported</td><td>Conversation</td><td>Tencent_hunyuan</td><td>The Hunyuan-large model has a total of about 389B parameters, with about 52B activated parameters, making it the open-source MoE model with the largest parameter scale and best performance in the industry.</td></tr><tr><td>hunyuan-large-longcontext</td><td>128k</td><td>6k</td><td>Not Supported</td><td>Conversation</td><td>Tencent_hunyuan</td><td>Excels at handling long-text tasks such as document summarization and document Q&A, while also being capable of handling general text generation tasks. It performs excellently in the analysis and generation of long texts, effectively handling complex and detailed long-form content processing needs.</td></tr><tr><td>hunyuan-lite</td><td>250k</td><td>6k</td><td>Not Supported</td><td>Conversation</td><td>Tencent_hunyuan</td><td>Upgraded to an MOE structure with a 256k context window, leading many open-source models in NLP, code, math, and industry-specific evaluation sets.</td></tr><tr><td>hunyuan-pro</td><td>28k</td><td>4k</td><td>Supported</td><td>Conversation</td><td>Tencent_hunyuan</td><td>A trillion-parameter scale MOE-32K long-text model. It achieves an absolute leading level on various benchmarks, with complex instruction and reasoning capabilities, complex mathematical abilities, and supports functioncall. It is specially optimized for applications in multilingual translation, finance, law, and medicine.</td></tr><tr><td>hunyuan-role</td><td>28k</td><td>4k</td><td>Not Supported</td><td>Conversation</td><td>Tencent_hunyuan</td><td>Hunyuan's latest role-playing model. This is a role-playing model officially fine-tuned and launched by Hunyuan, based on the Hunyuan model and augmented with role-playing scenario datasets, providing better foundational performance in role-playing scenarios.</td></tr><tr><td>hunyuan-standard</td><td>30k</td><td>2k</td><td>Not Supported</td><td>Conversation</td><td>Tencent_hunyuan</td><td>Adopts a better routing strategy, while also alleviating the problems of load balancing and expert convergence.<br>MOE-32K has a relatively higher cost-performance ratio and can handle long text inputs while balancing performance and price.</td></tr><tr><td>hunyuan-standard-256K</td><td>250k</td><td>6k</td><td>Not Supported</td><td>Conversation</td><td>Tencent_hunyuan</td><td>Adopts a better routing strategy, while also alleviating the problems of load balancing and expert convergence. For long texts, the "needle in a haystack" metric reaches 99.9%. MOE-256K further breaks through in length and performance, greatly expanding the input length.</td></tr><tr><td>hunyuan-translation-lite</td><td>4k</td><td>4k</td><td>Not Supported</td><td>Conversation</td><td>Tencent_hunyuan</td><td>The Hunyuan translation model supports natural language conversational translation; it supports mutual translation between Chinese and 15 languages including English, Japanese, French, Portuguese, Spanish, Turkish, Russian, Arabic, Korean, Italian, German, Vietnamese, Malay, and Indonesian.</td></tr><tr><td>hunyuan-turbo</td><td>28k</td><td>4k</td><td>Supported</td><td>Conversation</td><td>Tencent_hunyuan</td><td>The default version of the Hunyuan-turbo model, which uses a new Mixture-of-Experts (MoE) structure, resulting in faster inference efficiency and stronger performance compared to hunyuan-pro.</td></tr><tr><td>hunyuan-turbo-latest</td><td>28k</td><td>4k</td><td>Supported</td><td>Conversation</td><td>Tencent_hunyuan</td><td>The dynamically updated version of the Hunyuan-turbo model. It is the best-performing version in the Hunyuan model series, consistent with the C-end (Tencent Yuanbao).</td></tr><tr><td>hunyuan-turbo-vision</td><td>8k</td><td>2k</td><td>Supported</td><td>Vision, Conversation</td><td>Tencent_hunyuan</td><td>Hunyuan's new generation flagship visual language model, using a new Mixture-of-Experts (MoE) structure. Its capabilities in basic recognition, content creation, knowledge Q&A, and analysis/reasoning related to image-text understanding are comprehensively improved compared to the previous generation model. Max input 6k, max output 2k.</td></tr><tr><td>hunyuan-vision</td><td>8k</td><td>2k</td><td>Supported</td><td>Conversation, Vision</td><td>Tencent_hunyuan</td><td>Hunyuan's latest multimodal model, supporting image + text input to generate text content.<br>Basic Image Recognition: Recognizes subjects, elements, scenes, etc., in images.<br>Image Content Creation: Summarizes images, creates advertising copy, social media posts, poems, etc.<br>Multi-turn Image Dialogue: Engages in multi-turn interactive Q&A about a single image.<br>Image Analysis and Reasoning: Performs statistical analysis on logical relationships, math problems, code, and charts in images.<br>Image Knowledge Q&A: Answers questions about knowledge points contained in images, such as historical events, movie posters.<br>Image OCR: Recognizes text in images from natural life scenes and non-natural scenes.</td></tr><tr><td>SparkDesk-Lite</td><td>4k</td><td>-</td><td>Not Supported</td><td>Conversation</td><td>Spark_SparkDesk</td><td>Supports online web search function, with fast and convenient responses, suitable for low-power inference and model fine-tuning and other customized scenarios.</td></tr><tr><td>SparkDesk-Max</td><td>128k</td><td>-</td><td>Supported</td><td>Conversation</td><td>Spark_SparkDesk</td><td>Quantized from the latest Spark Large Model Engine 4.0 Turbo. It supports multiple built-in plugins such as web search, weather, and date. Core capabilities are fully upgraded, with universal improvements in application effects across various scenarios. Supports System role persona and FunctionCall.</td></tr><tr><td>SparkDesk-Max-32k</td><td>32k</td><td>-</td><td>Supported</td><td>Conversation</td><td>Spark_SparkDesk</td><td>Stronger reasoning: Enhanced context understanding and logical reasoning abilities. Longer input: Supports 32K tokens of text input, suitable for long document reading, private knowledge Q&A, and other scenarios.</td></tr><tr><td>SparkDesk-Pro</td><td>128k</td><td>-</td><td>Not Supported</td><td>Conversation</td><td>Spark_SparkDesk</td><td>Specially optimized for scenarios such as math, code, medicine, and education. Supports multiple built-in plugins like web search, weather, and date, covering most knowledge Q&A, language understanding, and text creation scenarios.</td></tr><tr><td>SparkDesk-Pro-128K</td><td>128k</td><td>-</td><td>Not Supported</td><td>Conversation</td><td>Spark_SparkDesk</td><td>Professional-grade large language model with tens of billions of parameters. It has been specially optimized for scenarios in medicine, education, and code, with lower latency in search scenarios. Suitable for business scenarios that have higher requirements for performance and response speed, such as text and intelligent Q&A.</td></tr><tr><td>moonshot-v1-128k</td><td>128k</td><td>4k</td><td>Supported</td><td>Conversation</td><td>Moonshot AI_moonshot</td><td>A model with a length of 8k, suitable for generating short text.</td></tr><tr><td>moonshot-v1-32k</td><td>32k</td><td>4k</td><td>Supported</td><td>Conversation</td><td>Moonshot AI_moonshot</td><td>A model with a length of 32k, suitable for generating long text.</td></tr><tr><td>moonshot-v1-8k</td><td>8k</td><td>4k</td><td>Supported</td><td>Conversation</td><td>Moonshot AI_moonshot</td><td>A model with a length of 128k, suitable for generating ultra-long text.</td></tr><tr><td>codegeex-4</td><td>128k</td><td>4k</td><td>Not Supported</td><td>Conversation, Code</td><td>Zhipu_codegeex</td><td>Zhipu's code model: suitable for automatic code completion tasks.</td></tr><tr><td>charglm-3</td><td>4k</td><td>2k</td><td>Not Supported</td><td>Conversation</td><td>Zhipu_glm</td><td>Persona model.</td></tr><tr><td>emohaa</td><td>8k</td><td>4k</td><td>Not Supported</td><td>Conversation</td><td>Zhipu_glm</td><td>Psychology model: possesses professional counseling abilities to help users understand emotions and cope with emotional problems.</td></tr><tr><td>glm-3-turbo</td><td>128k</td><td>4k</td><td>Not Supported</td><td>Conversation</td><td>Zhipu_glm</td><td>To be deprecated (June 30, 2025).</td></tr><tr><td>glm-4</td><td>128k</td><td>4k</td><td>Supported</td><td>Conversation</td><td>Zhipu_glm</td><td>Old flagship: released on January 16, 2024, now replaced by GLM-4-0520.</td></tr><tr><td>glm-4-0520</td><td>128k</td><td>4k</td><td>Supported</td><td>Conversation</td><td>Zhipu_glm</td><td>High-intelligence model: suitable for handling highly complex and diverse tasks.</td></tr><tr><td>glm-4-air</td><td>128k</td><td>4k</td><td>Supported</td><td>Conversation</td><td>Zhipu_glm</td><td>High cost-performance: the most balanced model between inference capability and price.</td></tr><tr><td>glm-4-airx</td><td>8k</td><td>4k</td><td>Supported</td><td>Conversation</td><td>Zhipu_glm</td><td>Extremely fast inference: has ultra-fast inference speed and powerful inference effects.</td></tr><tr><td>glm-4-flash</td><td>128k</td><td>4k</td><td>Supported</td><td>Conversation</td><td>Zhipu_glm</td><td>High speed, low price: ultra-fast inference speed.</td></tr><tr><td>glm-4-flashx</td><td>128k</td><td>4k</td><td>Supported</td><td>Conversation</td><td>Zhipu_glm</td><td>High speed, low price: Enhanced version of Flash, ultra-fast inference speed.</td></tr><tr><td>glm-4-long</td><td>1m</td><td>4k</td><td>Supported</td><td>Conversation</td><td>Zhipu_glm</td><td>Ultra-long input: specially designed for handling ultra-long text and memory-intensive tasks.</td></tr><tr><td>glm-4-plus</td><td>128k</td><td>4k</td><td>Supported</td><td>Conversation</td><td>Zhipu_glm</td><td>High-intelligence flagship: comprehensive performance improvement, with significantly enhanced long-text and complex task capabilities.</td></tr><tr><td>glm-4v</td><td>2k</td><td>-</td><td>Not Supported</td><td>Conversation, Vision</td><td>Zhipu_glm</td><td>Image understanding: possesses image understanding and reasoning capabilities.</td></tr><tr><td>glm-4v-flash</td><td>2k</td><td>1k</td><td>Not Supported</td><td>Conversation, Vision</td><td>Zhipu_glm</td><td>Free model: possesses powerful image understanding capabilities.</td></tr></tbody></table>