
{% hint style="warning" %}
이 문서는 AI에 의해 중국어에서 번역되었으며 아직 검토되지 않았습니다。
{% endhint %}

# Ollama

Ollama는 뛰어난 오픈소스 도구로, 다양한 대규모 언어 모델(LLM)을 로컬에서 쉽게 실행하고 관리할 수 있게 해줍니다. Cherry Studio는 이제 Ollama 통합을 지원하여, 익숙한 인터페이스에서 클라우드 서비스 의존 없이 로컬에 배포된 LLM과 직접 상호작용할 수 있습니다!

## Ollama란 무엇인가요?

Ollama는 대규모 언어 모델(LLM)의 배포와 사용을 단순화하는 도구로, 다음과 같은 특징을 가집니다:

* **로컬 실행:** 모델이 완전히 로컬 컴퓨터에서 실행되어 인터넷 연결이 필요 없으며, 개인정보와 데이터 보안이 보호됩니다.
* **간편한 사용:** 간단한 커맨드라인 명령어로 다양한 LLM을 다운로드, 실행 및 관리할 수 있습니다.
* **풍부한 모델 지원:** Llama 2, Deepseek, Mistral, Gemma 등 다양한 인기 오픈소스 모델을 지원합니다.
* **크로스 플랫폼:** macOS, Windows, Linux 시스템을 모두 지원합니다.
* **개방형 API:** OpenAI 호환 인터페이스를 지원해 다른 도구와 통합 가능합니다.

## Cherry Studio에서 Ollama를 사용해야 하는 이유

* **클라우드 서비스 불필요:** 클라우드 API의 할당량과 비용 제한 없이 로컬 LLM의 강력한 성능을 마음껏 경험할 수 있습니다.
* **데이터 프라이버시:** 모든 대화 데이터가 로컬에 유지되어 개인정보 유출 우려가 없습니다.
* **오프라인 사용 가능:** 네트워크 연결이 없는 상태에서도 LLM과 상호작용을 계속할 수 있습니다.
* **맞춤화:** 사용자 요구에 따라 가장 적합한 LLM을 선택하고 구성할 수 있습니다.

## Cherry Studio에서 Ollama 설정하기

### **1. Ollama 설치 및 실행**

먼저 컴퓨터에 Ollama를 설치하고 실행해야 합니다. 다음 단계를 따르세요:

*   **Ollama 다운로드:** Ollama 공식 웹사이트([https://ollama.com/](https://ollama.com/))에서 운영체제에 맞는 설치 패키지를 다운로드하세요.\
    Linux에서는 다음 명령어로 바로 설치할 수 있습니다:

    ```sh
    curl -fsSL https://ollama.com/install.sh | sh
    ```
* **Ollama 설치:** 설치 프로그램 안내에 따라 설치를 완료하세요.
*   **모델 다운로드:** 터미널(또는 명령 프롬프트)을 열고 `ollama run` 명령어로 사용할 모델을 다운로드하세요. 예를 들어 Llama 2 모델을 다운로드하려면:

    ```sh
    ollama run llama3.2
    ```

    Ollama가 자동으로 해당 모델을 다운로드하고 실행합니다.
* **Ollama 실행 유지:** Cherry Studio에서 Ollama 모델과 상호작용하는 동안 Ollama가 계속 실행 상태로 유지되도록 하세요.

### **2. Cherry Studio에 Ollama 서비스 공급자 추가**

다음으로, Cherry Studio에서 Ollama를 사용자 지정 AI 서비스 공급자로 추가하세요:

* **설정 열기:** Cherry Studio 인터페이스 왼쪽 탐색 바에서 "설정"(기어 아이콘)을 클릭하세요.
* **모델 서비스 진입:** 설정 페이지에서 "모델 서비스" 탭을 선택하세요.
* **공급자 추가:** 목록에서 Ollama를 클릭하세요.

<figure><img src="../../.gitbook/assets/image (5) (3).png" alt=""><figcaption></figcaption></figure>

### **3. Ollama 서비스 공급자 설정**

방금 추가한 Ollama 서비스 공급자를 찾아 상세 설정을 진행하세요:

1. **활성화 상태:**
   * 오른쪽에 있는 Ollama 서비스 공급자의 토글 스위치가 켜져 있는지 확인하세요(활성화 상태 표시).
2. **API 키:**
   * Ollama는 기본적으로 API 키가 **필요하지 않습니다**. 이 필드는 비워두거나 아무 내용이나 입력하세요.
3. **API 주소:**
   *    Ollama가 제공하는 로컬 API 주소를 입력하세요. 일반적으로 다음 주소를 사용합니다:

       ```
       http://localhost:11434/
       ```

       포트를 변경한 경우 수동으로 수정하세요.
4. **유지 활성 시간:** 이 옵션은 세션 유지 시간(분 단위)을 설정합니다. 설정한 시간 내에 새 대화가 없으면 Cherry Studio가 자동으로 Ollama 연결을 종료하여 리소스를 해제합니다.
5. **모델 관리:**
   * "+ 추가" 버튼을 클릭해 Ollama에서 이미 다운로드한 모델 이름을 수동으로 추가하세요.
   * `ollama run llama3.2`로 `llama3.2` 모델을 다운로드한 경우, 여기에 `llama3.2`를 입력하세요.
   * "관리" 버튼을 클릭해 추가된 모델을 수정하거나 삭제할 수 있습니다.

## 사용 시작하기

위 설정을 완료하면 Cherry Studio의 채팅 인터페이스에서 Ollama 서비스 공급자와 다운로드한 모델을 선택해 로컬 LLM과 대화를 시작할 수 있습니다!

## 팁과 활용법

* **모델 첫 실행:** 특정 모델을 처음 실행할 때 Ollama가 모델 파일을 다운로드하므로 시간이 오래 걸릴 수 있으며, 이때는 기다려 주세요.
* **사용 가능한 모델 확인:** 터미널에서 `ollama list` 명령어를 실행해 다운로드한 Ollama 모델 목록을 확인하세요.
* **하드웨어 요구사항:** 대규모 언어 모델 실행에는 어느 정도의 컴퓨팅 리소스(CPU, 메모리, GPU)가 필요하므로, 컴퓨터 사양이 모델 요구사항을 충족하는지 확인하세요.
* **Ollama 문서:** 설정 페이지의 `Ollama 문서 및 모델 보기` 링크를 클릭해 Ollama 공식 문서로 바로 이동할 수 있습니다.