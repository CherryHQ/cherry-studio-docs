
{% hint style="warning" %}
이 문서는 AI에 의해 중국어에서 번역되었으며 아직 검토되지 않았습니다。
{% endhint %}

# 사용자 정의 서비스 제공자

Cherry Studio는 주류 AI 모델 서비스를 통합했을 뿐만 아니라, 강력한 사용자 정의 기능을 제공합니다. **사용자 정의 AI 서비스 제공자** 기능을 통해 필요한 AI 모델을 손쉽게 연결할 수 있습니다.

## 사용자 정의 AI 서비스 제공자가 필요한 이유

* **유연성:** 미리 설정된 서비스 제공자 목록에 제한받지 않고, 필요에 가장 적합한 AI 모델을 자유롭게 선택할 수 있습니다.
* **다양성:** 다양한 플랫폼의 AI 모델을 시도해보고 그들의 고유한 장점을 발견할 수 있습니다.
* **제어 가능성:** 직접 API 키와 접근 주소를 관리하여 보안과 프라이버시를 보장합니다.
* **맞춤화:** 특정 비즈니스 시나리오의 요구를 충족시키기 위해 사설화 배포된 모델을 연결할 수 있습니다.

## 사용자 정의 AI 서비스 제공자를 추가하는 방법

간단한 몇 단계만으로 Cherry Studio에서 사용자 정의 AI 서비스 제공자를 추가할 수 있습니다:

<figure><img src="../../.gitbook/assets/image (2) (5).png" alt=""><figcaption></figcaption></figure>

1. **설정 열기:** Cherry Studio 인터페이스 왼쪽 내비게이션 바에서 "설정"(기어 아이콘)을 클릭합니다.
2. **모델 서비스 진입:** 설정 페이지에서 "모델 서비스" 탭을 선택합니다.
3. **제공자 추가:** "모델 서비스" 페이지에서 기존 서비스 제공자 목록을 볼 수 있습니다. 목록 하단의 "+ 추가" 버튼을 클릭하여 "제공자 추가" 팝업을 엽니다.
4. **정보 입력:** 팝업에서 다음 정보를 입력해야 합니다:
   * **제공자 이름:** 사용자 정의 서비스 제공자를 위한 쉽게 식별할 수 있는 이름을 지정합니다(예: MyCustomOpenAI).
   * **제공자 유형:** 드롭다운 목록에서 서비스 제공자 유형을 선택합니다. 현재 지원되는 유형:
     * OpenAI
     * Gemini
     * Anthropic
     * Azure OpenAI
5. **설정 저장:** 입력을 마친 후 "추가" 버튼을 클릭하여 구성을 저장합니다.

## 사용자 정의 AI 서비스 제공자 구성

<figure><img src="../../.gitbook/assets/image (3) (5) (1).png" alt=""><figcaption></figcaption></figure>

추가한 후에는 목록에서 방금 추가한 서비스 제공자를 찾아 세부 구성을 해야 합니다:

1. **활성화 상태** 사용자 정의 서비스 제공자 목록 가장 오른쪽에 있는 활성화 스위치를 켜서 해당 서비스를 활성화합니다.
2. **API 키:**
   * AI 서비스 제공자가 제공한 API 키(API Key)를 입력합니다.
   * 오른쪽의 "확인" 버튼을 클릭하면 키의 유효성을 검증할 수 있습니다.
3. **API 주소:**
   * AI 서비스의 API 접근 주소(Base URL)를 입력합니다.
   * 반드시 AI 서비스 제공자의 공식 문서를 참고하여 올바른 API 주소를 확인하세요.
4.  **모델 관리:**
    * "+ 추가" 버튼을 클릭하여 이 제공자에서 사용할 모델 ID(예: `gpt-3.5-turbo`, `gemini-pro` 등)를 수동으로 추가합니다.

    <figure><img src="../../.gitbook/assets/image (4) (5).png" alt=""><figcaption></figcaption></figure>

    * 구체적인 모델 이름이 확실하지 않다면 AI 서비스 제공자의 공식 문서를 참조하십시오.
    * "관리" 버튼을 클릭하면 이미 추가된 모델을 편집하거나 삭제할 수 있습니다.

## 시작하기

위 구성을 완료한 후, Cherry Studio의 채팅 인터페이스에서 사용자 정의 AI 서비스 제공자와 모델을 선택하여 AI와 대화를 시작할 수 있습니다!

## vLLM을 사용자 정의 AI 서비스 제공자로 사용하기

vLLM은 Ollama와 유사한 빠르고 사용하기 쉬운 LLM 추론 라이브러리입니다. 다음은 vLLM을 Cherry Studio에 통합하는 방법입니다:

1.  **vLLM 설치:** vLLM 공식 문서(https://docs.vllm.ai/en/latest/getting_started/quickstart.html)에 따라 vLLM을 설치합니다.

    ```sh
    pip install vllm # pip을 사용하는 경우
    uv pip install vllm # uv를 사용하는 경우
    ```
2.  **vLLM 서비스 시작:** vLLM에서 제공하는 OpenAI 호환 인터페이스를 사용하여 서비스를 시작합니다. 주로 다음 두 가지 방법이 있습니다:

    * `vllm.entrypoints.openai.api_server` 사용하여 시작

    ```sh
    python -m vllm.entrypoints.openai.api_server --model gpt2
    ```

    * `uvicorn`을 사용하여 시작

    ```sh
    vllm --model gpt2 --served-model-name gpt2
    ```

서비스가 성공적으로 시작되고 기본 포트 `8000`에서 수신 대기하는지 확인하십시오. 물론, `--port` 매개변수를 사용하여 vLLM 서비스의 포트 번호를 지정할 수도 있습니다.

3. **Cherry Studio에 vLLM 서비스 제공자 추가:**
   * 앞에서 설명한 단계에 따라 Cherry Studio에 새로운 사용자 정의 AI 서비스 제공자를 추가합니다.
   * **제공자 이름:** `vLLM`
   * **제공자 유형:** `OpenAI`를 선택합니다.
4. **vLLM 서비스 제공자 구성:**
   * **API 키:** vLLM은 API 키가 필요하지 않으므로, 이 필드를 비워두거나 아무 내용이나 입력할 수 있습니다.
   * **API 주소:** vLLM 서비스의 API 주소를 입력합니다. 기본적으로 주소는 `http://localhost:8000/`입니다(다른 포트를 사용한 경우 그에 맞게 수정).
   * **모델 관리:** vLLM에서 로드한 모델 이름을 추가합니다. 위의 `python -m vllm.entrypoints.openai.api_server --model gpt2` 실행 예시에서는 여기에 `gpt2`를 입력해야 합니다.
5. **대화 시작:** 이제 Cherry Studio에서 vLLM 서비스 제공자와 `gpt2` 모델을 선택하여 vLLM으로 구동되는 LLM과 대화를 시작할 수 있습니다!

## 팁과 요령

* **문서를 주의 깊게 읽으세요:** 사용자 정의 서비스 제공자를 추가하기 전에 반드시 사용하는 AI 서비스 제공자의 공식 문서를 주의 깊게 읽고 API 키, 접근 주소, 모델 이름 등 중요한 정보를 숙지하세요.
* **API 키 확인:** "확인" 버튼을 사용하여 API 키의 유효성을 빠르게 검증할 수 있으며, 키 오류로 인해 사용할 수 없게 되는 것을 방지합니다.
* **API 주소에 주의하세요:** 다른 AI 서비스 제공자와 모델은 API 주소가 다를 수 있으므로 반드시 올바른 주소를 입력하세요.
* **필요에 따라 모델 추가:** 실제로 사용할 모델만 추가하고, 너무 많은 불필요한 모델을 추가하지 마세요.