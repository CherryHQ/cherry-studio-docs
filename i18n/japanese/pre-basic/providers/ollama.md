
{% hint style="warning" %}
このドキュメントはAIによって中国語から翻訳されており、まだレビューされていません。
{% endhint %}

# Ollama

Ollamaは優れたオープンソースツールで、様々な大規模言語モデル（LLMs）をローカル環境で簡単に実行・管理できます。Cherry Studioは現在Ollama連携をサポートしており、使い慣れたインターフェースからクラウドサービスに依存せず、ローカルにデプロイしたLLMと直接対話できます！

## Ollamaとは？

Ollamaは大規模言語モデル（LLM）のデプロイと使用を簡素化するツールです。以下の特徴があります：

* **ローカル実行：** モデルは完全にローカルマシン上で動作するため、インターネット接続が不要で、プライバシーとデータセキュリティが保護されます。
* **簡単操作：** シンプルなコマンドライン指令で様々なLLMのダウンロード・実行・管理が可能です。
* **豊富なモデル：** Llama 2、Deepseek、Mistral、Gemmaなど様々な人気オープンソースモデルをサポート。
* **クロスプラットフォーム：** macOS、Windows、Linuxシステムをサポート。
* **オープンAPI：** OpenAI互換インターフェースをサポートし、他のツールとの統合が可能。

## Cherry StudioでOllamaを使用する理由

* **クラウド不要：** クラウドAPIのクォータ制限や費用制約から解放され、ローカルLLMの強力な機能を存分に体験できます。
* **データプライバシー：** すべての会話データがローカルに保持されるため、プライバシー漏洩の心配がありません。
* **オフライン利用可能：** ネットワーク接続がない環境でもLLMとの対話を継続できます。
* **カスタマイズ性：** ニーズに応じて最適なLLMを選択・設定できます。

## Cherry StudioでOllamaを設定する手順

### **1. Ollamaのインストールと起動**

最初に、コンピュータにOllamaをインストールして実行する必要があります：

* **Ollamaをダウンロード：** [Ollama公式サイト](https://ollama.com/) にアクセスし、ご使用のOSに対応したインストールパッケージをダウンロードしてください。\
  Linux環境では、次のコマンドで直接インストールできます：
  
  ```sh
  curl -fsSL https://ollama.com/install.sh | sh
  ```
* **Ollamaをインストール：** インストーラーの指示に従ってインストールを完了します。
* **モデルをダウンロード：** ターミナル（またはコマンドプロンプト）を開き、`ollama run` コマンドで使用したいモデルをダウンロードします。例えばLlama 2モデルをダウンロードするには：
  
  ```sh
  ollama run llama3.2
  ```
  
  Ollamaが自動的にモデルをダウンロード・起動します。
* **Ollamaの実行状態を維持：** Cherry StudioでOllamaモデルと対話する間は、Ollamaが実行中であることを確認してください。

### **2. Cherry StudioでOllamaプロバイダーを追加**

次に、Cherry StudioでカスタムAIプロバイダーとしてOllamaを追加します：

* **設定を開く：** Cherry Studioインターフェース左ナビゲーションバーで「設定」（歯車アイコン）をクリック。
* **モデルサービスに移動：** 設定ページで「モデルサービス」タブを選択。
* **プロバイダーを追加：** リスト中のOllamaをクリック。

<figure><img src="../../.gitbook/assets/image (5) (3).png" alt=""><figcaption></figcaption></figure>

### **3. Ollamaプロバイダーを設定**

サービスプロバイダーリストから追加したOllamaを見つけ、詳細設定を行います：

1. **有効状態：**
   * Ollamaプロバイダー右端のスイッチがオン（有効状態）になっていることを確認。
2. **APIキー：**
   * OllamaはデフォルトでAPIキーを**不要**とします。この欄は空欄のままか、任意の内容を入力。
3. **APIアドレス：**
   * Ollamaが提供するローカルAPIアドレスを入力。通常は以下のアドレス：
  
     ```
     http://localhost:11434/
     ```
  
     ポートを変更した場合は適宜修正。
4. **キープアライブ時間：** セッションの維持時間を設定（単位：分）。設定時間内に新しい対話がない場合、Cherry Studioは自動的にOllamaへの接続を切断してリソースを解放。
5. **モデル管理：**
   * 「+ 追加」ボタンをクリックし、Ollamaにダウンロード済みのモデル名を手動で追加。
   * 例えば `ollama run llama3.2` で `llama3.2` モデルをダウンロード済みの場合、ここに `llama3.2` と入力。
   * 「管理」ボタンをクリックすると、追加済みモデルの編集・削除が可能。

## 使い始め

上記設定完了後、Cherry StudioのチャットインターフェースでOllamaプロバイダーとダウンロード済みモデルを選択し、ローカルLLMとの対話を開始できます！

## テクニックとヒント

* **初回モデル実行：** モデル初回実行時はOllamaがモデルファイルをダウンロードするため時間がかかる場合があります。
* **利用可能モデルの確認：** ターミナルで `ollama list` コマンドを実行すると、ダウンロード済みOllamaモデルリストを確認可能。
* **ハードウェア要件：** 大規模言語モデル実行には一定の計算リソース（CPU、メモリ、GPU）が必要。コンピュータのスペックがモデル要件を満たしていることを確認。
* **Ollamaドキュメント：** 設定ページ中の `Ollamaドキュメントとモデルを表示` リンクをクリックすると、Ollama公式ドキュメントに即時アクセス可能。