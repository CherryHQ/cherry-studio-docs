# Ollama


{% hint style="warning" %}
このドキュメントはAIによって中国語から翻訳されており、まだレビューされていません。
{% endhint %}




Ollamaは優れたオープンソースツールで、ローカル環境でさまざまな大規模言語モデル（LLMs）を簡単に実行・管理できます。Cherry StudioはOllama統合をサポートしており、慣れ親しんだインターフェースでローカルにデプロイされたLLMと直接対話でき、クラウドサービスへの依存が不要です！

## Ollamaとは？

Ollamaは大規模言語モデル（LLM）のデプロイと使用を簡素化するツールです。主な特徴：

* **ローカル実行：** モデルは完全にローカルマシン上で動作し、インターネット接続不要でプライバシーとデータセキュリティを保護
* **シンプル操作：** 簡単なコマンドライン指令で多様なLLMのダウンロード・実行・管理が可能
* **豊富なモデル：** Llama 2、Deepseek、Mistral、Gemmaなど人気のオープンソースモデルをサポート
* **クロスプラットフォーム：** macOS、Windows、Linuxシステムに対応
* **オープンAPI：** OpenAI互換インターフェースをサポートし他ツールとの統合が可能

## Cherry StudioでOllamaを使う理由

* **クラウド不要：** クラウドAPIの制限や費用から解放され、ローカルLLMの能力を最大限活用
* **データプライバシー：** 全ての対話データがローカルに残り、情報漏洩の心配なし
* **オフライン対応：** ネットワーク接続がない環境でもLLMとの対話を継続可能
* **カスタマイズ性：** ニーズに最適なLLMを選択・設定できる柔軟性

## Cherry StudioでのOllama設定

### **1. Ollamaのインストールと実行**

まずローカルマシンにOllamaをインストールし実行します：

*   **Ollamaダウンロード：** 公式サイト（[https://ollama.com/](https://ollama.com/)）からOS対応のインストーラーを取得\
    Linux環境では次のコマンドで直接インストール可能：
    ```sh
    curl -fsSL https://ollama.com/install.sh | sh
    ```
* **インストール実行：** インストーラーの指示に従いセットアップ
*   **モデル取得：** ターミナルで`ollama run`コマンドを実行（例：Llama 2モデル取得）：
    ```sh
    ollama run llama3.2
    ```
    ※Ollamaが自動的にモデルをダウンロード・起動
* **Ollamaの常時稼働：** Cherry StudioでOllamaモデルを使用中はOllamaを実行状態に維持

### **2. Cherry StudioへのOllamaプロバイダ追加**

カスタムAIプロバイダとしてOllamaを追加：

* **設定画面を開く：** 左ナビゲーションバーの「設定」（歯車アイコン）をクリック
* **モデルサービスへ移動：** 設定ページで「モデルサービス」タブを選択
* **プロバイダ追加：** リスト内のOllamaをクリック

<figure><img src="../../.gitbook/assets/image (5) (3).png" alt=""><figcaption></figcaption></figure>

### **3. Ollamaプロバイダ設定**

追加したOllamaプロバイダを詳細設定：

1. **有効状態：**
   * 右端のトグルスイッチをONにして有効化
2. **APIキー：**
   * デフォルトでは**不要**なため空白のままか任意の値を入力
3. **APIアドレス：**
   *   ローカルAPIアドレスを入力（通常は以下）：
       ```
       http://localhost:11434/
       ```
       ※ポート変更時は適宜修正
4. **キープアライブ時間：** 分単位のセッション維持時間（設定時間内に対話がない場合、自動切断でリソース解放）
5. **モデル管理：**
   * 「+ 追加」ボタンでOllamaにダウンロード済みのモデル名を手動登録
   * 例：`ollama run llama3.2`で取得した`llama3.2`モデルなら`llama3.2`と入力
   * 「管理」ボタンで登録モデルの編集・削除が可能

## 使用方法

設定完了後、Cherry Studioのチャット画面でOllamaプロバイダとダウンロード済みモデルを選択し、ローカルLLMとの対話を開始できます！

## ヒントと注意点

* **初回モデル実行：** 初回実行時はモデルダウンロードに時間がかかる場合あり（完了までお待ちください）
* **利用可能モデル確認：** ターーミナルで`ollama list`コマンドを実行するとダウンロード済みモデルを一覧表示
* **ハードウェア要件：** LLM実行には十分な計算資源（CPU/メモリ/GPU）が必要（モデル要件を満たすマシン環境を確保）
* **Ollamaドキュメント：** 設定画面の`Ollamaドキュメントとモデルを表示`リンクから公式ドキュメントへ直接アクセス可能