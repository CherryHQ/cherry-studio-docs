
{% hint style="warning" %}
このドキュメントはAIによって中国語から翻訳されており、まだレビューされていません。
{% endhint %}

# Ollama

Ollamaは優れたオープンソースツールで、ローカル環境で様々な大規模言語モデル（LLMs）を簡単に実行・管理できます。Cherry StudioはOllamaとの統合をサポートし、使い慣れたインターフェースからローカルにデプロイされたLLMと直接対話可能。クラウドサービスへの依存が不要です！

## Ollamaとは？

Ollamaは大規模言語モデル（LLM）のデプロイメントと使用を簡素化するツールです。主な特徴：

* **ローカル実行：** モデルは完全にローカルマシン上で動作し、ネットワーク接続不要、プライバシーとデータセキュリティを保護
* **簡単操作：** シンプルなコマンドライン指令で様々なLLMのダウンロード・実行・管理が可能
* **豊富なモデル：** Llama 2、Deepseek、Mistral、Gemmaなど人気オープンソースモデルを多数サポート
* **クロスプラットフォーム：** macOS、Windows、Linuxシステムに対応
* **オープンAPI：** OpenAI互換インターフェースをサポートし、他のツールとの統合が可能

## Cherry StudioでOllamaを使う理由

* **クラウドサービス不要：** クラウドAPIの制限や費用から解放され、ローカルLLMの強力な機能を満喫
* **データプライバシー：** 全ての対話データがローカルに保存され、プライバシー漏洩の心配なし
* **オフライン利用可能：** ネットワーク接続がない環境でもLLMとの対話を継続可能
* **カスタマイズ性：** ニーズに応じて最適なLLMを選択・設定可能

## Cherry StudioでのOllama設定方法

### **1. Ollamaのインストールと実行**

まずコンピューターにOllamaをインストールし実行する必要があります：

* **Ollamaのダウンロード：** Ollama公式サイト（[https://ollama.com/](https://ollama.com/)）にアクセスし、OSに合わせてインストーラーをダウンロード  
  Linux環境では次のコマンドで直接インストール可能：

  ```sh
  curl -fsSL https://ollama.com/install.sh | sh
  ```
* **Ollamaのインストール：** インストーラーの指示に従ってインストール完了
* **モデルのダウンロード：** ターミナルで`ollama run`コマンドを使用し、必要なモデルをダウンロード  
  例：Llama 3.2モデルをダウンロードする場合：

  ```sh
  ollama run llama3.2
  ```
  Ollamaが自動的にモデルをダウンロード・実行します
* **Ollamaの実行状態維持：** Cherry StudioでOllamaモデルを使用する間は、Ollamaが実行状態であることを確認してください

### **2. Cherry StudioへのOllamaプロバイダー追加**

次に、OllamaをカスタムAIプロバイダーとして追加：

* **設定を開く：** Cherry Studioインターフェース左側ナビゲーションバーの「設定」（歯車アイコン）をクリック
* **モデルサービスへ移動：** 設定ページで「モデルサービス」タブを選択
* **プロバイダーを追加：** リスト中のOllamaをクリック

<figure><img src="../../.gitbook/assets/image (5) (3).png" alt=""><figcaption></figcaption></figure>

### **3. Ollamaプロバイダーの設定**

追加したOllamaプロバイダーを詳細設定：

1. **有効状態：**
   * Ollamaプロバイダー右端のスイッチがオン（有効状態）であることを確認
2. **APIキー：**
   * OllamaはデフォルトでAPIキーを**必要としません**。このフィールドは空欄のまま、または任意の内容を入力可
3. **APIアドレス：**
   *   Ollamaが提供するローカルAPIアドレスを入力。通常は次のアドレス：
       ```
       http://localhost:11434/
       ```
       ポート番号を変更した場合は適宜修正
4. **キープアライブ時間：** セッションの保持時間を分単位で設定。設定時間内に新しい対話がない場合、Cherry Studioは自動的にOllamaとの接続を切断しリソースを解放
5. **モデル管理：**
   * 「+ 追加」ボタンをクリックし、Ollamaでダウンロード済みのモデル名を手動入力
   * 例：`ollama run llama3.2`で`llama3.2`モデルをダウンロードした場合、ここに`llama3.2`と入力
   * 「管理」ボタンで追加済みモデルの編集・削除が可能

## 使い始め

上記設定完了後、Cherry StudioのチャットインターフェースでOllamaプロバイダーとダウンロード済みモデルを選択し、ローカルLLMとの対話を開始できます！

## テクニックとヒント

* **モデル初回実行時：** 初めて特定のモデルを実行する場合、Ollamaはモデルファイルをダウンロードするため時間がかかります。しばらくお待ちください
* **利用可能モデルの確認：** ターミナルで`ollama list`コマンドを実行すると、ダウンロード済みOllamaモデルを一覧表示可能
* **ハードウェア要件：** 大規模言語モデルの実行には相当な計算リソース（CPU、メモリ、GPU）が必要です。お使いのコンピューターがモデル要件を満たしていることを確認ください
* **Ollamaドキュメント：** 設定ページの`Ollamaドキュメントとモデルを表示`リンクをクリックすると、Ollama公式ドキュメントに直接アクセス可能