
{% hint style="warning" %}
Αυτό το έγγραφο μεταφράστηκε από τα Κινεζικά με AI και δεν έχει ακόμη ελεγχθεί.
{% endhint %}

# Ollama

Το Ollama είναι ένα εξαιρετικό εργαλείο ανοιχτού κώδικα που σας επιτρέπει να εκτελείτε και να διαχειρίζεστε εύκολα διάφορα μεγάλα γλωσσικά μοντέλα (LLMs) τοπικά. Το Cherry Studio υποστηρίζει πλέον την ενσωμάτωση του Ollama, επιτρέποντάς σας να αλληλεπιδράτε με LLM που έχουν αναπτυχθεί τοπικά μέσα από τη γνωστή διεπαφή, χωρίς να χρειάζεστε υπηρεσίες cloud!

## Τι είναι το Ollama;

Το Ollama είναι ένα εργαλείο που απλοποιεί την ανάπτυξη και τη χρήση μεγάλων γλωσσικών μοντέλων (LLMs). Έχει τα ακόλουθα χαρακτηριστικά:

* **Τοπική εκτέλεση:** Τα μοντέλα εκτελούνται πλήρως στον τοπικό σας υπολογιστή, χωρίς απαίτηση σύνδεσης στο διαδίκτυο, προστατεύοντας το απόρρητό σας και την ασφάλεια των δεδομένων.
* **Απλό και εύκολο στη χρήση:** Μέσω απλών εντολών γραμμής εντολών, μπορείτε να κατεβάζετε, να εκτελείτε και να διαχειρίζεστε διάφορα LLM.
* **Πλούσιο σύνολο μοντέλων:** Υποστηρίζει δημοφιλή ανοιχτού κώδικα μοντέλα όπως Llama 2, Deepseek, Mistral, Gemma και πολλά άλλα.
* **Διαπλατφορμικό:** Υποστηρίζει συστήματα macOS, Windows και Linux.
* **Ανοιχτό API:** Υποστηρίζει διεπαφή συμβατή με το OpenAI, επιτρέποντάς την ενσωμάτωση με άλλα εργαλεία.

## Γιατί να χρησιμοποιήσετε το Ollama στο Cherry Studio;

* **Χωρίς υπηρεσίες cloud:** Δεν περιορίζεστε πλέον από τα όρια και το κόστος των API στο cloud, απολαμβάνοντας πλήρως τις δυνατότητες των τοπικών LLM.
* **Απόρρητο δεδομένων:** Όλα τα δεδομένα των συζητήσεών σας παραμένουν τοπικά, χωρίς ανησυχίες για διαρροή απορρήτου.
* **Διαθέσιμο εκτός σύνδεσης:** Μπορείτε να συνεχίσετε την αλληλεπίδραση με τα LLM ακόμα και χωρίς σύνδεση στο διαδίκτυο.
* **Εξατομίκευση:** Μπορείτε να επιλέγετε και να ρυθμίζετε το LLM που ταιριάζει καλύτερα στις ανάγκες σας.

## Ρύθμιση του Ollama στο Cherry Studio

### **1. Εγκατάσταση και εκτέλεση του Ollama**

Αρχικά, πρέπει να εγκαταστήσετε και να εκτελέσετε το Ollama στον υπολογιστή σας. Ακολουθήστε τα παρακάτω βήματα:

* **Λήψη του Ollama:** Επισκεφθείτε τον επίσημο ιστότοπο του Ollama ([https://ollama.com/](https://ollama.com/)) και κατεβάστε το πακέτο εγκατάστασης για το λειτουργικό σας σύστημα.\
  Σε Linux, μπορείτε να εκτελέσετε την παρακάτω εντολή για να το εγκαταστήσετε:

  ```sh
  curl -fsSL https://ollama.com/install.sh | sh
  ```
* **Εγκατάσταση του Ollama:** Ολοκληρώστε την εγκατάσταση ακολουθώντας τις οδηγίες του εγκαταστάτη.
* **Λήψη μοντέλου:** Ανοίξτε το τερματικό (ή την γραμμή εντολών) και χρησιμοποιήστε την εντολή `ollama run` για να κατεβάσετε το μοντέλο που θέλετε να χρησιμοποιήσετε. Για παράδειγμα, για να κατεβάσετε το μοντέλο Llama 2, εκτελέστε:

  ```sh
  ollama run llama3.2
  ```

  Το Ollama θα κατεβάσει και θα εκτελέσει αυτόματα το μοντέλο.
* **Διατήρηση εκτέλεσης του Ollama:** Κατά τη διάρκεια της αλληλεπίδρασής σας με τα μοντέλα του Ollama μέσω του Cherry Studio, βεβαιωθείτε ότι το Ollama παραμένει σε λειτουργία.

### **2. Προσθήκη του Ollama ως πάροχου υπηρεσιών στο Cherry Studio**

Στη συνέχεια, προσθέστε το Ollama ως προσαρμοσμένο πάροχο AI υπηρεσιών στο Cherry Studio:

* **Άνοιγμα ρυθμίσεων:** Στην αριστερή μπάρα πλοήγησης του περιβάλλοντος του Cherry Studio, κάντε κλικ στο "Ρυθμίσεις" (εικονίδιο γραναζιού).
* **Μετάβαση σε υπηρεσίες μοντέλων:** Στη σελίδα ρυθμίσεων, επιλέξτε την καρτέλα "Υπηρεσίες μοντέλων".
* **Προσθήκη παρόχου:** Κάντε κλικ στο Ollama στη λίστα.

<figure><img src="../../.gitbook/assets/image (5) (3).png" alt=""><figcaption></figcaption></figure>

### **3. Ρύθμιση του πάροχου υπηρεσιών Ollama**

Βρείτε τον πάροχο Ollama που μόλις προσθέσατε στη λίστα και πραγματοποιήστε λεπτομερείς ρυθμίσεις:

1. **Κατάσταση ενεργοποίησης:**
   * Βεβαιωθείτε ότι ο διακόπτης στα δεξιά του πάροχου Ollama είναι ανοιχτός, γεγονός που υποδεικνύει ότι είναι ενεργοποιημένος.
2. **Κλειδί API:**
   * Το Ollama **δεν απαιτεί** κλειδί API από προεπιλογή. Μπορείτε να αφήσετε αυτό το πεδίο κενό ή να συμπληρώσετε οτιδήποτε.
3. **Διεύθυνση API:**
   * Συμπληρώστε την τοπική διεύθυνση API που παρέχει το Ollama. Συνήθως, η διεύθυνση είναι:

     ```
     http://localhost:11434/
     ```

     Εάν έχετε αλλάξει τη θύρα, αλλάξτε την ανάλογα.
4. **Διάρκεια διατήρησης σύνδεσης:**
   * Αυτή η επιλογή καθορίζει τη διάρκεια διατήρησης της συνεδρίας σε λεπτά. Εάν δεν υπάρξει νέα συνομιλία εντός της καθορισμένης διάρκειας, το Cherry Studio θα διακόψει αυτόματα τη σύνδεση με το Ollama για να απελευθερώσει πόρους.
5. **Διαχείριση μοντέλων:**
   * Κάντε κλικ στο κουμπί "+ Προσθήκη" για να προσθέσετε χειροκίνητα το όνομα του μοντέλου που έχετε ήδη κατεβάσει στο Ollama.
   * Για παράδειγμα, εάν έχετε κατεβάσει το μοντέλο `llama3.2` μέσω της εντολής `ollama run llama3.2`, τότε εδώ μπορείτε να εισάγετε `llama3.2`
   * Κάνοντας κλικ στο κουμπί "Διαχείριση", μπορείτε να επεξεργαστείτε ή να διαγράψετε τα ήδη προστεθέντα μοντέλα.

## Ξεκινώντας

Μετά την ολοκλήρωση των παραπάνω ρυθμίσεων, μπορείτε να επιλέξετε τον πάροχο υπηρεσιών Ollama και το μοντέλο που έχετε κατεβάσει στη διεπαφή συνομιλίας του Cherry Studio για να ξεκινήσετε τη συνομιλία με το τοπικό LLM!

## Συμβουλές και υποδείξεις

* **Πρώτη εκτέλεση μοντέλου:** Κατά την πρώτη εκτέλεση ενός μοντέλου, το Ollama χρειάζεται να κατεβάσει τα αρχεία του μοντέλου, πράγμα που μπορεί να απαιτήσει αρκετό χρόνο. Παρακαλούμε για υπομονή.
* **Προβολή διαθέσιμων μοντέλων:** Εκτελέστε την εντολή `ollama list` στο τερματικό για να δείτε τη λίστα των μοντέλων Ollama που έχετε κατεβάσει.
* **Απαιτήσεις υλικού:** Η εκτέλεση μεγάλων γλωσσικών μοντέλων απαιτεί σημαντικούς υπολογιστικούς πόρους (CPU, μνήμη, GPU). Βεβαιωθείτε ότι η διαμόρφωση του υπολογιστή σας πληροί τις απαιτήσεις του μοντέλου.
* **Τεκμηρίωση Ollama:** Μπορείτε να κάνετε κλικ στον σύνδεσμο `View Ollama documentation and models` στη σελίδα ρυθμίσεων για να μεταβείτε γρήγορα στην επίσημη τεκμηρίωση του Ollama.