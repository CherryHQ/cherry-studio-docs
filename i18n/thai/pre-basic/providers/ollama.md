
{% hint style="warning" %}
เอกสารนี้ได้รับการแปลจากภาษาจีนโดย AI และยังไม่ได้รับการตรวจสอบ
{% endhint %}

# Ollama

Ollama เป็นเครื่องมือโอเพนซอร์สที่ยอดเยี่ยม ช่วยให้คุณสามารถเรียกใช้งานและจัดการโมเดลภาษาขนาดใหญ่ (LLMs) ต่างๆ บนเครื่องของคุณได้อย่างง่ายดาย ปัจจุบัน Cherry Studio ได้รองรับการผสานรวมกับ Ollama แล้ว ช่วยให้คุณสามารถโต้ตอบกับ LLM ที่ติดตั้งในเครื่องได้ทันทีผ่านอินเทอร์เฟซที่คุ้นเคย โดยไม่ต้องพึ่งพาบริการคลาวด์!

## Ollama คืออะไร?

Ollama เป็นเครื่องมือที่ช่วยลดความซับซ้อนในการปรับใช้และใช้งานโมเดลภาษาขนาดใหญ่ (LLM) โดยมีคุณสมบัติเด่นดังนี้:

* **ทำงานในเครื่อง:** โมเดลทำงานทั้งหมดบนคอมพิวเตอร์ส่วนตัวของคุณโดยไม่ต้องเชื่อมต่ออินเทอร์เน็ต ช่วยปกป้องความเป็นส่วนตัวและความปลอดภัยของข้อมูล
* **ใช้งานง่าย:** ดาวน์โหลด เรียกใช้งาน และจัดการ LLMs ต่างๆ ได้ด้วยคำสั่งบรรทัดคำสั่งง่ายๆ
* **โมเดลหลากหลาย:** รองรับโมเดลโอเพนซอร์สยอดนิยมหลากหลายประเภท เช่น Llama 2, Deepseek, Mistral, Gemma
* **ข้ามแพลตฟอร์ม:** รองรับระบบปฏิบัติการ macOS, Windows และ Linux
* **OpenAPI:** รองรับอินเทอร์เฟซที่เข้ากันได้กับ OpenAI สามารถผสานรวมกับเครื่องมืออื่นๆ ได้

## ทำไมต้องใช้ Ollama ใน Cherry Studio?

* **ไม่ต้องพึ่งคลาวด์:** ใช้งานฟังก์ชันล้ำหน้าของ LLM ในเครื่องได้อย่างเต็มที่ โดยไม่จำกัดด้วยโควต้าหรือค่าใช้จ่าย API คลาวด์
* **ความเป็นส่วนตัวของข้อมูล:** ข้อมูลบทสนทนาทั้งหมดของคุณยังคงอยู่ในเครื่อง ไม่ต้องกังวลเรื่องการรั่วไหลของข้อมูลส่วนตัว
* **ใช้งานแบบออฟไลน์:** สามารถโต้ตอบกับ LLM ต่อไปได้แม้ไม่มีเครือข่ายอินเทอร์เน็ต
* **ปรับแต่งได้:** สามารถเลือกและกำหนดค่า LLM ที่เหมาะกับความต้องการของคุณที่สุดได้

## การกำหนดค่า Ollama ใน Cherry Studio

### **1. ติดตั้งและเรียกใช้ Ollama**

ก่อนอื่น คุณต้องติดตั้งและเรียกใช้ Ollama บนคอมพิวเตอร์ของคุณ โดยทำตามขั้นตอนดังนี้:

*   **ดาวน์โหลด Ollama:** ไปที่เว็บไซต์ Ollama ([https://ollama.com/](https://ollama.com/)) ดาวน์โหลดแพ็คเกจติดตั้งที่ตรงกับระบบปฏิบัติการของคุณ\
    สำหรับ Linux สามารถติดตั้งผ่านคำสั่งโดยตรง:

    ```sh
    curl -fsSL https://ollama.com/install.sh | sh
    ```
* **ติดตั้ง Ollama:** ทำตามคำแนะนำของโปรแกรมติดตั้งให้เสร็จสมบูรณ์
*   **ดาวน์โหลดโมเดล:** เปิดเทอร์มินัล (หรือ Command Prompt) และใช้คำสั่ง `ollama run` เพื่อดาวน์โหลดโมเดลที่ต้องการใช้งาน เช่น หากต้องการดาวน์โหลดโมเดล Llama 2 ให้รัน:

    ```sh
    ollama run llama3.2
    ```

    Ollama จะดาวน์โหลดและเรียกใช้โมเดลนี้โดยอัตโนมัติ
* **ให้ Ollama ทำงานตลอด:** ขณะที่คุณใช้งาน Cherry Studio เพื่อโต้ตอบกับโมเดล Ollama ต้องแน่ใจว่า Ollama ยังคงทำงานอยู่

### **2. เพิ่มผู้ให้บริการ Ollama ใน Cherry Studio**

ต่อไป ให้เพิ่ม Ollama เป็นผู้ให้บริการ AI แบบกำหนดเองใน Cherry Studio:

* **เปิดการตั้งค่า:** คลิกไอคอน "การตั้งค่า" (รูปร่างเฟือง) ในแถบนำด้านซ้ายของอินเทอร์เฟซ Cherry Studio
* **เข้าสู่บริการโมเดล:** ในหน้าตั้งค่า ให้เลือกแท็บ "บริการโมเดล" (Model Services)
* **เพิ่มผู้ให้บริการ:** คลิกที่ Ollama ในรายการ

<figure><img src="../../.gitbook/assets/image (5) (3).png" alt=""><figcaption></figcaption></figure>

### **3. กำหนดค่ารายละเอียดผู้ให้บริการ Ollama**

ค้นหาผู้ให้บริการ Ollama ที่เพิ่งเพิ่มในรายการ และกำหนดค่ารายละเอียด:

1. **สถานะการเปิดใช้งาน:**
   * ต้องแน่ใจว่าสวิตช์ทางด้านขวาของผู้ให้บริการ Ollama อยู่ในตำแหน่งเปิด (เปิดใช้งาน)
2. **คีย์ API:**
   * Ollama เริ่มต้น**ไม่จำเป็นต้องใช้**คีย์ API คุณสามารถปล่อยช่องนี้ว่างไว้หรือป้อนข้อมูลอะไรก็ได้
3. **ที่อยู่ API:**
   * ป้อนที่อยู่ API ในเครื่องที่ให้โดย Ollama โดยปกติจะเป็น:

       ```
       http://localhost:11434/
       ```

       หากมีการเปลี่ยนพอร์ต โปรดแก้ไขตามความเหมาะสม
4. **ระยะเวลาดำเนินการต่อเนื่อง:** ตัวเลือกนี้กำหนดระยะเวลาในการรักษาบทสนทนา (หน่วยเป็นนาที) หากไม่มีการสนทนาใหม่ภายในเวลาที่กำหนด Cherry Studio จะตัดการเชื่อมต่อกับ Ollama โดยอัตโนมัติเพื่อปล่อยทรัพยากร
5. **การจัดการโมเดล:**
   * คลิกปุ่ม "+ เพิ่ม" เพื่อเพิ่มชื่อโมเดลที่คุณดาวน์โหลดไว้ใน Ollama ด้วยตนเอง
   * เช่น หากคุณเคยดาวน์โหลดโมเดล `llama3.2` ผ่าน `ollama run llama3.2` คุณสามารถป้อน `llama3.2` ในช่องนี้
   * คลิกปุ่ม "จัดการ" เพื่อแก้ไขหรือลบโมเดลที่เพิ่มแล้ว

## เริ่มต้นใช้งาน

หลังการกำหนดค่าข้างต้นเสร็จสมบูรณ์ คุณสามารถเลือกผู้ให้บริการ Ollama และโมเดลที่ดาวน์โหลดไว้แล้วในอินเทอร์เฟซการสนทนาของ Cherry Studio เพื่อเริ่มโต้ตอบกับ LLM ในเครื่องของคุณได้ทันที!

## เคล็ดลับและคำแนะนำ

* **การเรียกใช้โมเดลครั้งแรก:** การเรียกใช้โมเดลครั้งแรก Ollama จะต้องดาวน์โหลดไฟล์โมเดลซึ่งอาจใช้เวลานาน โปรดรออย่างอดทน
* **ตรวจสอบโมเดลที่มี:** รันคำสั่ง `ollama list` ในเทอร์มินัลเพื่อดูรายการโมเดล Ollama ที่คุณดาวน์โหลดไว้แล้ว
* **ข้อกำหนดฮาร์ดแวร์:** การเรียกใช้โมเดลภาษาขนาดใหญ่ต้องใช้ทรัพยากรคอมพิวเตอร์ที่เพียงพอ (CPU, RAM, GPU) โปรดตรวจสอบว่าคอมพิวเตอร์ของคุณมีคุณสมบัติตรงตามข้อกำหนดของโมเดล
* **เอกสาร Ollama:** สามารถคลิกที่ลิงก์ `ดูเอกสารและโมเดลของ Ollama` ในหน้าตั้งค่าเพื่อไปยังเอกสารเว็บไซต์ Ollama อย่างรวดเร็ว