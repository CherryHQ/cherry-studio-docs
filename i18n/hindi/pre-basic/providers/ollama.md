
{% hint style="warning" %}
यह दस्तावेज़ AI द्वारा चीनी से अनुवादित किया गया है और अभी तक इसकी समीक्षा नहीं की गई है।
{% endhint %}

# Ollama

Ollama एक उत्कृष्ट ओपन-सोर्स टूल है जो आपको स्थानीय रूप से विभिन्न बड़े भाषा मॉडल (LLMs) को आसानी से चलाने और प्रबंधित करने की सुविधा प्रदान करता है। Cherry Studio ने अब Ollama इंटीग्रेशन को सपोर्ट कर दिया है, जिससे आप परिचित इंटरफेस में सीधे स्थानीय रूप से डिप्लॉय किए गए LLM के साथ इंटरैक्ट कर सकते हैं, बिना क्लाउड सेवाओं पर निर्भर रहे!

## Ollama क्या है?

Ollama एक टूल है जो बड़े भाषा मॉडल (LLM) के डिप्लॉयमेंट और उपयोग को सरल बनाता है। इसकी मुख्य विशेषताएं हैं:

* **स्थानीय रन:** मॉडल पूरी तरह आपके स्थानीय कंप्यूटर पर चलता है, इंटरनेट कनेक्शन की आवश्यकता नहीं होती, जिससे आपकी गोपनीयता और डेटा सुरक्षा बनी रहती है।
* **उपयोग में आसान:** सरल कमांड लाइन निर्देशों के माध्यम से विभिन्न LLM को डाउनलोड, रन और मैनेज कर सकते हैं।
* **विविध मॉडल:** Llama 2, Deepseek, Mistral, Gemma जैसे कई लोकप्रिय ओपन-सोर्स मॉडल्स को सपोर्ट करता है।
* **क्रॉस-प्लेटफॉर्म:** macOS, Windows और Linux सिस्टम को सपोर्ट करता है।
* **ओपन एपीआई:** OpenAI-संगत इंटरफेस का समर्थन करता है, जिससे अन्य टूल्स के साथ इंटीग्रेट किया जा सकता है।

## Cherry Studio में Ollama का उपयोग क्यों करें?

* **क्लाउड सेवाओं की आवश्यकता नहीं:** क्लाउड API की कोटा और लागत सीमाओं से मुक्त होकर स्थानीय LLM की शक्तिशाली क्षमताओं का आनंद लें।
* **डेटा गोपनीयता:** आपका सारा संवाद डेटा स्थानीय स्तर पर ही रहता है, गोपनीयता भंग होने की चिंता न करें।
* **ऑफ़लाइन उपयोग:** नेटवर्क कनेक्शन के बिना भी LLM के साथ इंटरैक्ट करना जारी रख सकते हैं।
* **अनुकूलन:** अपनी आवश्यकताओं के अनुसार सबसे उपयुक्त LLM का चयन और कॉन्फ़िगर कर सकते हैं।

## Cherry Studio में Ollama को कॉन्फ़िगर करें

### **1. Ollama इंस्टॉल और रन करें**

सबसे पहले, आपको अपने कंप्यूटर पर Ollama इंस्टॉल और रन करना होगा। कृपया निम्नलिखित चरणों का पालन करें:

*   **Ollama डाउनलोड करें:** Ollama ऑफिसियल वेबसाइट ([https://ollama.com/](https://ollama.com/)) पर जाएं और अपने ऑपरेटिंग सिस्टम के अनुसार इंस्टॉलेशन पैकेज डाउनलोड करें।\
    Linux पर, निम्न कमांड चलाकर सीधे ollama इंस्टॉल कर सकते हैं:

    ```sh
    curl -fsSL https://ollama.com/install.sh | sh
    ```
* **Ollama इंस्टॉल करें:** इंस्टॉलेशन प्रोग्राम के निर्देशों का पालन करके इंस्टॉलेशन पूरा करें।
*   **मॉडल डाउनलोड करें:** टर्मिनल (या कमांड प्रॉम्प्ट) खोलें, और `ollama run` कमांड का उपयोग करके वह मॉडल डाउनलोड करें जिसका आप उपयोग करना चाहते हैं। उदाहरण के लिए, Llama 2 मॉडल डाउनलोड करने के लिए निम्न कमांड चलाएं:

    ```sh
    ollama run llama3.2
    ```

    Ollama स्वचालित रूप से इस मॉडल को डाउनलोड और रन करेगा।
* **Ollama को रनिंग रखें:** जब तक आप Cherry Studio के माध्यम से Ollama मॉडल के साथ इंटरैक्ट कर रहे हैं, तब तक Ollama को रनिंग स्टेट में रखें।

### **2. Cherry Studio में Ollama को कस्टम AI सेवा प्रदाता के रूप में जोड़ें**

अगला चरण, Cherry Studio में Ollama को कस्टम AI सेवा प्रदाता के रूप में जोड़ें:

* **सेटिंग्स खोलें:** Cherry Studio इंटरफेस के बाईं ओर नेविगेशन बार में, "सेटिंग्स" (गियर आइकन) पर क्लिक करें।
* **मॉडल सेवाएं में जाएं:** सेटिंग्स पेज पर, "मॉडल सेवाएं" टैब चुनें।
* **प्रदाता जोड़ें:** सूची में Ollama पर क्लिक करें।

<figure><img src="../../.gitbook/assets/image (5) (3).png" alt=""><figcaption></figcaption></figure>

### **3. Ollama सेवा प्रदाता को कॉन्फ़िगर करें**

सेवा प्रदाता सूची में नए जोड़े गए Ollama को ढूंढें और विस्तृत कॉन्फ़िगरेशन करें:

1. **सक्रियता स्थिति:**
   * सुनिश्चित करें कि Ollama सेवा प्रदाता के दाईं ओर का स्विच चालू है, जो इंगित करता है कि यह सक्षम है।
2. **API कुंजी:**
   * Ollama डिफ़ॉल्ट रूप से API कुंजी की **आवश्यकता नहीं** करता है। आप इस फ़ील्ड को खाली छोड़ सकते हैं, या कोई भी सामग्री भर सकते हैं।
3. **API पता:**
   *    O निम्नलिखित Ollama का स्थानीय API पता भरें। सामान्यतः, पता होता है:

       ```
       http://localhost:11434/
       ```

       यदि पोर्ट परिवर्तित किया गया है, तो उसे स्वयं अपडेट करें।
4. **सक्रिय रखने का समय:** यह विकल्प सत्र को बनाए रखने का समय सेट करता है, इकाई मिनटों में है। यदि निर्धारित समय में कोई नया संवाद नहीं होता है, तो Cherry Studio स्वचालित रूप से Ollama से कनेक्शन तोड़ देगा, संसाधनों को मुक्त करेगा।
5. **मॉडल प्रबंधन:**
   * "+ जोड़ें" बटन पर क्लिक करें, और Ollama में आपके द्वारा पहले से डाउनलोड किए गए मॉडल के नाम को मैन्युअल रूप से जोड़ें।
   * उदाहरण के लिए, यदि आपने `ollama run llama3.2` के माध्यम से `llama3.2` मॉडल डाउनलोड किया है, तो आप यहां `llama3.2` भर सकते हैं।
   * "प्रबंधित" बटन पर क्लिक करके, जोड़े गए मॉडल को संपादित या हटा सकते हैं।

## उपयोग शुरू करें

उपरोक्त कॉन्फ़िगरेशन पूरा होने के बाद, आप Cherry Studio के चैट इंटरफेस में Ollama सेवा प्रदाता और आपके डाउनलोड किए गए मॉडल का चयन करके स्थानीय LLM के साथ बातचीत शुरू कर सकते हैं!

## टिप्स और संकेत

* **मॉडल को पहली बार चलाना:** किसी मॉडल को पहली बार चलाते समय, Ollama को मॉडल फ़ाइलें डाउनलोड करनी पड़ती हैं, जिसमें अधिक समय लग सकता है - कृपया धैर्य रखें।
* **उपलब्ध मॉडल देखना:** टर्मिनल में `ollama list` कमांड चलाकर, आपके द्वारा डाउनलोड किए गए Ollama मॉडल्स की सूची देख सकते हैं।
* **हार्डवेयर आवश्यकताएं:** बड़े भाषा मॉडल चलाने के लिए पर्याप्त कंप्यूटिंग संसाधनों (CPU, मेमोरी, GPU) की आवश्यकता होती है - कृपया सुनिश्चित करें कि आपका कंप्यूटर कॉन्फ़िगरेशन मॉडल की आवश्यकताओं को पूरा करता है।
* **Ollama डॉक्यूमेंटेशन:** कॉन्फ़िगरेशन पेज में `Ollama डॉक्यूमेंटेशन और मॉडल देखें` लिंक पर क्लिक करके Ollama ऑफिसियल डॉक्यूमेंटेशन पर सीधे जा सकते हैं।